<!DOCTYPE html>
<html>
  <head>
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1">
<title> Models | CS 109a Final Project</title>
<meta name="description" content="">
<meta name="keywords" content="">
<link rel="stylesheet" href="/lora/css/main.css">
<meta name="HandheldFriendly" content="True">
<meta name="MobileOptimized" content="320">
<meta name="theme-color" content="#2ecc71">
<link rel="canonical" href="/lora/models/">
<link rel="alternate" type="application/rss+xml" title="CS 109a Final Project" href="/lora/feed.xml" />
<script src=”//ajax.googleapis.com/ajax/libs/jquery/1.11.0/jquery.min.js”></script>
<script type="text/javascript">
var _gaq = _gaq || [];
_gaq.push(['_setAccount', '']);
_gaq.push(['_trackPageview']);
(function() {
  var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
  ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
  var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
})();
</script>
  </head>
  <body class="page">
    <main class="container">
        <header class="site-header">
  <div class="container txt-center">
    <nav class="navbar">
      <ul>
        <li><a href="/lora/">Background</a></li>
        <li class="divider">&bull;</li>
        <li><a href="/lora/scrape">Data Scraping</a></li>
        <li class="divider">&bull;</li>
        <li><a href="/lora/eda">EDA</a></li>
        <li class="divider">&bull;</li>
        <li><a href="/lora/models">Models</a></li>
        <li class="divider">&bull;</li>
        <li><a href="/lora/conclusions">Conclusions</a></li>
      </ul>
    </nav>
    <hr class="hr">
  </div>
</header>
        <section class="main-content">
          <article class="post">
  <header class="post-header">
    <h1 class="post-title">Models</h1>
    <p class="post-meta"></p>
  </header>
  <div class="post-content">
    <h2 class="no_toc" id="contents">Contents</h2>
<ul id="markdown-toc">
  <li><a href="#model-construction" id="markdown-toc-model-construction">3 Model Construction</a>    <ul>
      <li><a href="#basic-prediction-models" id="markdown-toc-basic-prediction-models">3.1 Basic Prediction Models</a>        <ul>
          <li><a href="#load-dataframes-and-cursory-cleaning" id="markdown-toc-load-dataframes-and-cursory-cleaning">3.1.1 Load Dataframes and cursory cleaning</a></li>
          <li><a href="#set-up-training-and-validation-datasets" id="markdown-toc-set-up-training-and-validation-datasets">3.1.2 Set up training and validation datasets</a></li>
          <li><a href="#linear-ridge-lasso-knn-regression-cv-on-basic-columns-sex-income-etc" id="markdown-toc-linear-ridge-lasso-knn-regression-cv-on-basic-columns-sex-income-etc">3.1.3 Linear Ridge, Lasso, KNN regression CV on basic columns (sex, income, etc…)</a></li>
        </ul>
      </li>
      <li><a href="#include-more-comprehensive-income-levels-in-each-model" id="markdown-toc-include-more-comprehensive-income-levels-in-each-model">3.1.4 Include more comprehensive income levels in each model</a>        <ul>
          <li><a href="#include-all-columns-in-each-model" id="markdown-toc-include-all-columns-in-each-model">3.1.5 Include all columns in each model</a></li>
        </ul>
      </li>
      <li><a href="#subset-selection" id="markdown-toc-subset-selection">3.2 Subset Selection</a>        <ul>
          <li><a href="#ols-model-to-determine-statistically-significant-predictors" id="markdown-toc-ols-model-to-determine-statistically-significant-predictors">3.2.1 OLS Model to determine statistically significant predictors</a></li>
          <li><a href="#train-models-on-most-significant-predictors-determined-by-ols" id="markdown-toc-train-models-on-most-significant-predictors-determined-by-ols">3.2.2 Train models on most significant predictors determined by OLS</a></li>
          <li><a href="#stepwise-bic-subset-selection" id="markdown-toc-stepwise-bic-subset-selection">3.2.3 Stepwise BIC Subset Selection</a></li>
          <li><a href="#use-columns-from-forwards-and-backwards-selection-in-ridge-lasso-k-nn-regression" id="markdown-toc-use-columns-from-forwards-and-backwards-selection-in-ridge-lasso-k-nn-regression">3.2.4 Use columns from forwards and backwards selection in Ridge, Lasso, K-NN Regression</a></li>
          <li><a href="#polynomial-features-and-interaction-variables" id="markdown-toc-polynomial-features-and-interaction-variables">3.2.5 Polynomial Features and Interaction Variables</a></li>
        </ul>
      </li>
      <li><a href="#model-trained-with-atf-data" id="markdown-toc-model-trained-with-atf-data">3.3 Model Trained with ATF Data</a>        <ul>
          <li><a href="#retrieve-atf-data-and-merge-dfs" id="markdown-toc-retrieve-atf-data-and-merge-dfs">3.3.1 Retrieve ATF Data and merge DFs</a></li>
          <li><a href="#linear-ridge-lasso-and-knn-regressors-trained-on-all-columns-including-atf" id="markdown-toc-linear-ridge-lasso-and-knn-regressors-trained-on-all-columns-including-atf">3.3.2 Linear, Ridge, Lasso, and KNN Regressors trained on all columns including ATF</a></li>
          <li><a href="#ols-model-to-determine-statistical-significance-of-additional-atf-columns" id="markdown-toc-ols-model-to-determine-statistical-significance-of-additional-atf-columns">3.3.3 OLS Model to determine statistical significance of additional ATF columns</a></li>
          <li><a href="#models-trained-on-most-significant-predictors-determined-by-the-ols-model-with-atf-columns" id="markdown-toc-models-trained-on-most-significant-predictors-determined-by-the-ols-model-with-atf-columns">3.3.4 Models trained on most significant predictors determined by the OLS Model with ATF Columns</a></li>
          <li><a href="#models-trained-on-most-significant-predictors-with-polynomial-and-interaction-variables" id="markdown-toc-models-trained-on-most-significant-predictors-with-polynomial-and-interaction-variables">3.3.5 Models trained on most significant predictors with polynomial and interaction variables</a></li>
          <li><a href="#forwards-and-backwards-subset-selection" id="markdown-toc-forwards-and-backwards-subset-selection">3.3.6 Forwards and Backwards subset selection</a></li>
        </ul>
      </li>
    </ul>
  </li>
  <li><a href="#testing-the-final-2-models" id="markdown-toc-testing-the-final-2-models">4 Testing the Final 2 models</a>    <ul>
      <li><a href="#lasso-regression-with-atf-data-using-predictors-with-p-stat--005-and-polynomial-features" id="markdown-toc-lasso-regression-with-atf-data-using-predictors-with-p-stat--005-and-polynomial-features">4.1 Lasso Regression with ATF data, using predictors with p stat &lt; 0.05 and polynomial features</a></li>
      <li><a href="#knn-regressor-with-forwards-and-backwards-selected-predictors" id="markdown-toc-knn-regressor-with-forwards-and-backwards-selected-predictors">4.2 KNN Regressor with forwards and backwards selected predictors</a></li>
    </ul>
  </li>
</ul>
<p><br /><br /></p>
<h2 id="model-construction">3 Model Construction</h2>
<h3 id="basic-prediction-models">3.1 Basic Prediction Models</h3>
<h4 id="load-dataframes-and-cursory-cleaning">3.1.1 Load Dataframes and cursory cleaning</h4>
<p>Our initial approach and trajectory of modeling was: first, create baseline models; second, go through subset selection for stronger performance; third, incorporate another dimension of data from the Bureau of Alcohol, Tobacco, and Firearms (ATF); and, finally, test our selected final model.</p>
<div class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">census_data_path</span> <span class="o">=</span> <span class="s">&#39;Crime/data/census/&#39;</span></code></pre></div>
<div class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">census_folder_fps</span> <span class="o">=</span> <span class="p">[</span><span class="n">f</span> <span class="k">for</span> <span class="n">f</span> <span class="ow">in</span> <span class="n">listdir</span><span class="p">(</span><span class="n">census_data_path</span><span class="p">)</span> <span class="k">if</span> <span class="n">isfile</span><span class="p">(</span><span class="n">join</span><span class="p">(</span><span class="n">census_data_path</span><span class="p">,</span> <span class="n">f</span><span class="p">))]</span></code></pre></div>
<div class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">csv_filepaths</span> <span class="o">=</span> <span class="p">[</span><span class="n">f</span> <span class="k">for</span> <span class="n">f</span> <span class="ow">in</span> <span class="n">listdir</span><span class="p">(</span><span class="s">&#39;csvs/&#39;</span><span class="p">)</span> <span class="k">if</span> <span class="n">isfile</span><span class="p">(</span><span class="n">join</span><span class="p">(</span><span class="s">&#39;csvs/&#39;</span><span class="p">,</span> <span class="n">f</span><span class="p">))]</span></code></pre></div>
<div class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">census_filepaths</span> <span class="o">=</span> <span class="p">[</span><span class="n">f</span> <span class="k">for</span> <span class="n">f</span> <span class="ow">in</span> <span class="n">census_folder_fps</span> <span class="k">if</span> <span class="n">f</span><span class="p">[</span><span class="mi">3</span><span class="p">:]</span> <span class="o">==</span> <span class="s">&#39;census.csv&#39;</span><span class="p">]</span></code></pre></div>
<div class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">def</span> <span class="nf">strip_dashes</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="s">&#39; &#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="s">&#39; &#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s">&#39;-&#39;</span><span class="p">))</span><span class="o">.</span><span class="n">split</span><span class="p">())</span></code></pre></div>
<div class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">def</span> <span class="nf">merge_df_msa</span><span class="p">(</span><span class="n">fbi_df</span><span class="p">,</span> <span class="n">census_df</span><span class="p">):</span>
    <span class="n">census_mask</span> <span class="o">=</span> <span class="n">census_df</span><span class="p">[</span><span class="s">&#39;Geography&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">isnull</span><span class="p">()</span>
    <span class="n">fbi_mask</span> <span class="o">=</span> <span class="n">fbi_df</span><span class="p">[</span><span class="s">&#39;Metropolitan_Statistical_Area&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">isnull</span><span class="p">()</span>
    <span class="n">df_c</span> <span class="o">=</span> <span class="n">census_df</span><span class="p">[</span><span class="o">~</span><span class="n">census_mask</span><span class="p">]</span>
    <span class="n">df_f</span> <span class="o">=</span> <span class="n">fbi_df</span><span class="p">[</span><span class="o">~</span><span class="n">fbi_mask</span><span class="p">]</span>
    <span class="n">geos</span> <span class="o">=</span> <span class="n">df_c</span><span class="p">[</span><span class="s">&#39;Geography&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s">&#39;Metro Area&#39;</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">strip</span><span class="p">())</span>
    <span class="n">geos</span> <span class="o">=</span> <span class="n">geos</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">strip_dashes</span><span class="p">)</span>
    <span class="n">df_c</span><span class="p">[</span><span class="s">&#39;Geography&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">geos</span>
    <span class="n">fbi_geos</span> <span class="o">=</span> <span class="n">df_f</span><span class="p">[</span><span class="s">&#39;Metropolitan_Statistical_Area&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">strip_dashes</span><span class="p">)</span>
    <span class="n">df_f</span><span class="p">[</span><span class="s">&#39;Metropolitan_Statistical_Area&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">fbi_geos</span>
    <span class="k">return</span> <span class="n">pd</span><span class="o">.</span><span class="n">merge</span><span class="p">(</span><span class="n">df_f</span><span class="p">,</span> <span class="n">df_c</span><span class="p">,</span> <span class="n">left_on</span><span class="o">=</span><span class="s">&#39;Metropolitan_Statistical_Area&#39;</span><span class="p">,</span> <span class="n">right_on</span><span class="o">=</span><span class="s">&#39;Geography&#39;</span><span class="p">,</span> <span class="n">how</span><span class="o">=</span><span class="s">&#39;inner&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="s">&#39;Geography&#39;</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span></code></pre></div>
<div class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">df_dict</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">()</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">11</span><span class="p">):</span>
    <span class="n">df_fbi</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="n">join</span><span class="p">(</span><span class="s">&#39;csvs/&#39;</span><span class="p">,</span> <span class="n">csv_filepaths</span><span class="p">[</span><span class="n">i</span><span class="p">]))</span>
    <span class="n">df_census</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="n">join</span><span class="p">(</span><span class="n">census_data_path</span><span class="p">,</span> <span class="n">census_filepaths</span><span class="p">[</span><span class="n">i</span><span class="p">]),</span> <span class="n">skiprows</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">drop</span><span class="p">([</span><span class="s">&#39;Id&#39;</span><span class="p">,</span> <span class="s">&#39;Id2&#39;</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="n">col</span> <span class="k">for</span> <span class="n">col</span> <span class="ow">in</span> <span class="n">df_census</span><span class="o">.</span><span class="n">columns</span> <span class="k">if</span> <span class="s">&#39;Native&#39;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">col</span> <span class="ow">and</span> <span class="s">&#39;Margin of Error&#39;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">col</span><span class="p">]</span>
    <span class="n">foreign</span> <span class="o">=</span> <span class="p">[</span><span class="n">x</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">df_census</span><span class="o">.</span><span class="n">columns</span> <span class="k">if</span> <span class="s">&#39;born outside&#39;</span> <span class="ow">in</span> <span class="n">x</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">non_native</span> <span class="o">=</span> <span class="n">df_census</span><span class="p">[</span><span class="n">foreign</span><span class="p">]</span>
    <span class="n">df_census</span> <span class="o">=</span> <span class="n">df_census</span><span class="p">[</span><span class="n">columns</span><span class="p">]</span>
    <span class="n">df_census</span><span class="p">[</span><span class="s">&#39;non_native&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">non_native</span>
    <span class="n">df</span> <span class="o">=</span> <span class="n">merge_df_msa</span><span class="p">(</span><span class="n">df_fbi</span><span class="p">,</span> <span class="n">df_census</span><span class="p">)</span>
    <span class="c"># Compute murders per 100,000 people</span>
    <span class="n">population</span> <span class="o">=</span> <span class="p">[</span><span class="n">x</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">df</span><span class="o">.</span><span class="n">columns</span> <span class="k">if</span> <span class="s">&#39;Total population&#39;</span> <span class="ow">in</span> <span class="n">x</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">per_100</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">to_numeric</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="n">population</span><span class="p">],</span> <span class="n">errors</span><span class="o">=</span><span class="s">&#39;coerce&#39;</span><span class="p">)</span><span class="o">/</span><span class="mf">100000.0</span>
    <span class="n">df</span><span class="p">[</span><span class="s">&#39;Murders_per_100000&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s">&#39;Murder_and_nonnegligent_manslaughter&#39;</span><span class="p">]</span><span class="o">/</span><span class="n">per_100</span>
    <span class="n">df</span><span class="p">[</span><span class="s">&#39;population&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="n">population</span><span class="p">]</span>
    <span class="c"># Drop any columns that don&#39;t have murder counts</span>
    <span class="n">df</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">dropna</span><span class="p">(</span><span class="n">subset</span><span class="o">=</span><span class="p">[</span><span class="s">&#39;Murders_per_100000&#39;</span><span class="p">])</span>
    <span class="n">df_dict</span><span class="p">[</span><span class="n">i</span> <span class="o">+</span> <span class="mi">6</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span></code></pre></div>
<div class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">frames</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">df_dict</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
    <span class="c"># Various labels we decided would be relevant, basic demographic</span>
    <span class="n">male</span> <span class="o">=</span> <span class="p">[</span><span class="n">x</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">v</span><span class="o">.</span><span class="n">columns</span> <span class="k">if</span> <span class="s">&#39;Male&#39;</span> <span class="ow">in</span> <span class="n">x</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">median_age</span> <span class="o">=</span> <span class="p">[</span><span class="n">x</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">v</span><span class="o">.</span><span class="n">columns</span> <span class="k">if</span> <span class="s">&#39;Median age&#39;</span> <span class="ow">in</span> <span class="n">x</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">median_income</span> <span class="o">=</span> <span class="p">[</span><span class="n">x</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">v</span><span class="o">.</span><span class="n">columns</span> <span class="k">if</span> <span class="s">&#39;Median income&#39;</span> <span class="ow">in</span> <span class="n">x</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">black</span> <span class="o">=</span> <span class="p">[</span><span class="n">x</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">v</span><span class="o">.</span><span class="n">columns</span> <span class="k">if</span> <span class="s">&#39;Black or&#39;</span> <span class="ow">in</span> <span class="n">x</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">white</span> <span class="o">=</span> <span class="p">[</span><span class="n">x</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">v</span><span class="o">.</span><span class="n">columns</span> <span class="k">if</span> <span class="s">&#39;White&#39;</span> <span class="ow">in</span> <span class="n">x</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">hispanic</span> <span class="o">=</span> <span class="p">[</span><span class="n">x</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">v</span><span class="o">.</span><span class="n">columns</span> <span class="k">if</span> <span class="s">&#39;Hispanic&#39;</span> <span class="ow">in</span> <span class="n">x</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">poverty</span> <span class="o">=</span> <span class="p">[</span><span class="n">x</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">v</span><span class="o">.</span><span class="n">columns</span> <span class="k">if</span> <span class="s">&#39;poverty&#39;</span> <span class="ow">in</span> <span class="n">x</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span>
    <span class="c"># Income Labels</span>
    <span class="n">income_levels</span> <span class="o">=</span> <span class="p">[</span><span class="n">x</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">v</span><span class="o">.</span><span class="n">columns</span> <span class="k">if</span> <span class="s">&#39;Population 15 years and over - $&#39;</span> <span class="ow">in</span> <span class="n">x</span><span class="p">]</span>
    <span class="n">income_labels</span> <span class="o">=</span> <span class="p">[</span><span class="n">x</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s">&quot;- &quot;</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">v</span><span class="o">.</span><span class="n">columns</span> <span class="k">if</span> <span class="s">&#39;Population 15 years and over - $&#39;</span> <span class="ow">in</span> <span class="n">x</span><span class="p">]</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">level</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">income_levels</span><span class="p">):</span>
        <span class="n">v</span><span class="p">[</span><span class="n">income_labels</span><span class="p">[</span><span class="n">i</span><span class="p">]]</span> <span class="o">=</span> <span class="n">v</span><span class="p">[</span><span class="n">income_levels</span><span class="p">[</span><span class="n">i</span><span class="p">]]</span>
    <span class="c"># Education Levels</span>
    <span class="n">uneducated</span> <span class="o">=</span> <span class="p">[</span><span class="n">x</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">v</span><span class="o">.</span><span class="n">columns</span> <span class="k">if</span> <span class="s">&#39;EDUCATIONAL ATTAINMENT&#39;</span> <span class="ow">in</span> <span class="n">x</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">highschool</span> <span class="o">=</span> <span class="p">[</span><span class="n">x</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">v</span><span class="o">.</span><span class="n">columns</span> <span class="k">if</span> <span class="s">&#39;EDUCATIONAL ATTAINMENT&#39;</span> <span class="ow">in</span> <span class="n">x</span><span class="p">][</span><span class="mi">2</span><span class="p">]</span>
    <span class="n">college</span> <span class="o">=</span> <span class="p">[</span><span class="n">x</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">v</span><span class="o">.</span><span class="n">columns</span> <span class="k">if</span> <span class="s">&#39;EDUCATIONAL ATTAINMENT&#39;</span> <span class="ow">in</span> <span class="n">x</span><span class="p">][</span><span class="mi">3</span><span class="p">:</span><span class="mi">5</span><span class="p">]</span>
    <span class="n">v</span><span class="p">[</span><span class="s">&quot;Year&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="mi">2000</span> <span class="o">+</span> <span class="n">k</span>
    <span class="n">non_native_series</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">to_numeric</span><span class="p">(</span><span class="n">v</span><span class="p">[</span><span class="s">&#39;non_native&#39;</span><span class="p">],</span> <span class="n">errors</span><span class="o">=</span><span class="s">&#39;coerce&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">fillna</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">pop_series</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">to_numeric</span><span class="p">(</span><span class="n">v</span><span class="p">[</span><span class="s">&#39;Total; Estimate; Total population&#39;</span><span class="p">],</span> <span class="n">errors</span><span class="o">=</span><span class="s">&#39;coerce&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">fillna</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">cols</span> <span class="o">=</span> <span class="p">[</span><span class="s">&#39;Murder_and_nonnegligent_manslaughter&#39;</span><span class="p">,</span> <span class="s">&#39;Murders_per_100000&#39;</span><span class="p">,</span> <span class="s">&#39;Year&#39;</span><span class="p">,</span> <span class="n">poverty</span><span class="p">,</span> <span class="n">male</span><span class="p">,</span>
            <span class="n">median_age</span><span class="p">,</span> <span class="n">median_income</span><span class="p">,</span> <span class="n">black</span><span class="p">,</span> <span class="n">white</span><span class="p">,</span> <span class="n">hispanic</span><span class="p">,</span> <span class="n">uneducated</span><span class="p">,</span> <span class="n">highschool</span><span class="p">]</span> <span class="o">+</span> <span class="n">income_levels</span>
    <span class="n">df_temp</span> <span class="o">=</span> <span class="n">v</span><span class="p">[</span><span class="n">cols</span><span class="p">]</span>
    <span class="n">column_labels</span> <span class="o">=</span> <span class="p">[</span><span class="s">&#39;murder_count&#39;</span><span class="p">,</span> <span class="s">&#39;murder_per_100000&#39;</span><span class="p">,</span> <span class="s">&#39;year&#39;</span><span class="p">,</span> <span class="s">&#39;poverty&#39;</span><span class="p">,</span> <span class="s">&#39;male&#39;</span><span class="p">,</span><span class="s">&#39;median_age&#39;</span><span class="p">,</span>
                       <span class="s">&#39;median_income&#39;</span><span class="p">,</span> <span class="s">&#39;black&#39;</span><span class="p">,</span> <span class="s">&#39;white&#39;</span><span class="p">,</span> <span class="s">&#39;hispanic&#39;</span><span class="p">,</span> <span class="s">&#39;uneducated&#39;</span><span class="p">,</span> <span class="s">&#39;highschool&#39;</span><span class="p">]</span> <span class="o">+</span> <span class="n">income_labels</span>
    <span class="c"># Percentage of population that are young adults, could include or not!</span>
    <span class="n">ya_tier</span> <span class="o">=</span> <span class="p">[</span><span class="n">x</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">v</span><span class="o">.</span><span class="n">columns</span> <span class="k">if</span> <span class="s">&#39;AGE&#39;</span> <span class="ow">in</span> <span class="n">x</span><span class="p">][:</span><span class="mi">8</span><span class="p">][</span><span class="mi">2</span><span class="p">]</span>
    <span class="n">ya_series</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">to_numeric</span><span class="p">(</span><span class="n">v</span><span class="p">[</span><span class="n">ya_tier</span><span class="p">],</span> <span class="n">errors</span><span class="o">=</span><span class="s">&#39;coerce&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">fillna</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">ya_percentage</span> <span class="o">=</span> <span class="n">ya_series</span><span class="o">/</span><span class="n">pop_series</span>
    <span class="n">df_temp</span><span class="o">.</span><span class="n">columns</span> <span class="o">=</span> <span class="n">column_labels</span>
    <span class="n">df_temp</span><span class="p">[</span><span class="s">&#39;college&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">v</span><span class="p">[</span><span class="n">college</span><span class="p">]</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">df_temp</span><span class="p">[</span><span class="s">&#39;foreign&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">non_native_series</span> <span class="o">/</span> <span class="n">pop_series</span>
    <span class="n">df_temp</span><span class="p">[</span><span class="s">&#39;population&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">v</span><span class="p">[</span><span class="s">&#39;population&#39;</span><span class="p">]</span>
    <span class="n">frames</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">df_temp</span><span class="p">)</span>
<span class="n">df_concat</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">concat</span><span class="p">(</span><span class="n">frames</span><span class="p">)</span></code></pre></div>
<div class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">y</span> <span class="o">=</span> <span class="n">df_concat</span><span class="p">[</span><span class="s">&#39;murder_per_100000&#39;</span><span class="p">]</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">df_concat</span><span class="o">.</span><span class="n">drop</span><span class="p">([</span><span class="s">&#39;murder_per_100000&#39;</span><span class="p">,</span> <span class="s">&#39;murder_count&#39;</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">pd</span><span class="o">.</span><span class="n">to_numeric</span><span class="p">,</span> <span class="n">errors</span><span class="o">=</span><span class="s">&#39;coerce&#39;</span><span class="p">)</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span></code></pre></div>
<h4 id="set-up-training-and-validation-datasets">3.1.2 Set up training and validation datasets</h4>
<p>First, in 3.1.1, we loaded the dataframes from FBI and Census files and split the data into training and testing sets with a fifth of the data being used for the test set. Since we plan to use the test set for the final evaluation of our selected model, we decided to create a separate validation set with which we would compare between the baseline models. We defined a function called ‘train_and_validate’ which trains and validates a linear regression, lasso and ridge regressions with cross validation, and a k-Nearest Neighbor classifier models on selected columns.</p>
<div class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">X_tr</span><span class="p">,</span> <span class="n">X_val</span><span class="p">,</span> <span class="n">y_tr</span><span class="p">,</span> <span class="n">y_val</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span></code></pre></div>
<div class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">def</span> <span class="nf">train_and_validate</span><span class="p">(</span><span class="n">X_tr</span><span class="p">,</span> <span class="n">X_val</span><span class="p">,</span> <span class="n">y_tr</span><span class="p">,</span> <span class="n">y_val</span><span class="p">,</span> <span class="n">cols</span><span class="p">,</span> <span class="n">title</span><span class="p">,</span> <span class="n">add_const</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">poly</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">include_knn</span><span class="o">=</span><span class="bp">True</span><span class="p">):</span>
    <span class="n">X_col_tr</span> <span class="o">=</span> <span class="n">X_tr</span><span class="p">[</span><span class="n">cols</span><span class="p">]</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">pd</span><span class="o">.</span><span class="n">to_numeric</span><span class="p">,</span> <span class="n">errors</span><span class="o">=</span><span class="s">&#39;coerce&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">values</span>
    <span class="n">X_col_val</span> <span class="o">=</span> <span class="n">X_val</span><span class="p">[</span><span class="n">cols</span><span class="p">]</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">pd</span><span class="o">.</span><span class="n">to_numeric</span><span class="p">,</span> <span class="n">errors</span><span class="o">=</span><span class="s">&#39;coerce&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">values</span>
    <span class="n">imputer</span> <span class="o">=</span> <span class="n">Imputer</span><span class="p">(</span><span class="n">strategy</span><span class="o">=</span><span class="s">&#39;mean&#39;</span><span class="p">)</span>
    <span class="c"># Fill nan values with means</span>
    <span class="n">X_col_tr</span> <span class="o">=</span> <span class="n">imputer</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X_col_tr</span><span class="p">)</span>
    <span class="n">X_col_val</span> <span class="o">=</span> <span class="n">imputer</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X_col_val</span><span class="p">)</span>
    <span class="c"># Scale everything</span>
    <span class="n">scaler</span> <span class="o">=</span> <span class="n">MinMaxScaler</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">poly</span><span class="p">:</span>
        <span class="n">poly_generator</span> <span class="o">=</span> <span class="n">PolynomialFeatures</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="n">include_bias</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">add_const</span><span class="p">:</span>
            <span class="n">X_col_tr</span> <span class="o">=</span> <span class="n">poly_generator</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">scaler</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X_col_tr</span><span class="p">))</span>
            <span class="n">X_col_val</span> <span class="o">=</span> <span class="n">poly_generator</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">scaler</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X_col_val</span><span class="p">))</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">X_col_tr</span> <span class="o">=</span> <span class="n">sm</span><span class="o">.</span><span class="n">add_constant</span><span class="p">(</span><span class="n">poly_generator</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">scaler</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X_tr_col</span><span class="p">)))</span>
            <span class="n">X_col_val</span> <span class="o">=</span> <span class="n">sm</span><span class="o">.</span><span class="n">add_constant</span><span class="p">(</span><span class="n">poly_generator</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">scaler</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X_val_col</span><span class="p">)))</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">add_const</span><span class="p">:</span>
            <span class="n">X_col_tr</span> <span class="o">=</span> <span class="n">sm</span><span class="o">.</span><span class="n">add_constant</span><span class="p">(</span><span class="n">scaler</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X_col_tr</span><span class="p">))</span>
            <span class="n">X_col_val</span> <span class="o">=</span> <span class="n">sm</span><span class="o">.</span><span class="n">add_constant</span><span class="p">(</span><span class="n">scaler</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X_col_val</span><span class="p">))</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">X_col_tr</span> <span class="o">=</span> <span class="n">scaler</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X_col_tr</span><span class="p">)</span>
            <span class="n">X_col_val</span> <span class="o">=</span> <span class="n">scaler</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X_col_val</span><span class="p">)</span></code></pre></div>
<h4 id="linear-ridge-lasso-knn-regression-cv-on-basic-columns-sex-income-etc">3.1.3 Linear Ridge, Lasso, KNN regression CV on basic columns (sex, income, etc…)</h4>
<p>For our most basic model, we selected the following columns to predict the crime rate in each MSA area: median age, median income, male demographic percentage, percentage of people living under poverty, and population. Since all of our predictor variables are numeric values with certain instances of NaN values, we imputed the missing values with the respective mens and, then, applied a scale to standardize these numeric representations. The three linear models unsurprisingly resulted in low training R-squared values of around 0.10, with validation R-squared values a just little shy of 0.10. However, the k-NN model with k of 13 provided very strong training R-squared value of 0.38 but returned very low validation score of 0.02, hinting at the possibility of overfitting on non-significant independent variables.</p>
<div class="highlight"><pre><code class="language-python" data-lang="python"><span class="c"># Linear Regression</span>
    <span class="n">lr</span> <span class="o">=</span> <span class="n">LinearRegression</span><span class="p">()</span>
    <span class="n">lr</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_col_tr</span><span class="p">,</span> <span class="n">y_tr</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="s">&#39;Training R^2 for {} Linear Regression: {}&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">title</span><span class="p">,</span> <span class="n">lr</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_col_tr</span><span class="p">,</span> <span class="n">y_tr</span><span class="p">)))</span>
    <span class="k">print</span><span class="p">(</span><span class="s">&#39;Validation R^2 for {} Linear Regression: {}&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">title</span><span class="p">,</span> <span class="n">lr</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_col_val</span><span class="p">,</span> <span class="n">y_val</span><span class="p">)))</span>
    <span class="k">print</span><span class="p">()</span>
    <span class="c"># Lasso Regression</span>
    <span class="n">lasso</span> <span class="o">=</span> <span class="n">LassoCV</span><span class="p">(</span><span class="n">cv</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
    <span class="n">lasso</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_col_tr</span><span class="p">,</span> <span class="n">y_tr</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="s">&#39;Training R^2 for {} Lasso Regression: {}&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">title</span><span class="p">,</span> <span class="n">lasso</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_col_tr</span><span class="p">,</span> <span class="n">y_tr</span><span class="p">)))</span>
    <span class="k">print</span><span class="p">(</span><span class="s">&#39;Validation R^2 for {} Lasso Regression: {}&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">title</span><span class="p">,</span> <span class="n">lasso</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_col_val</span><span class="p">,</span> <span class="n">y_val</span><span class="p">)))</span>
    <span class="k">print</span><span class="p">()</span>
    <span class="c"># Ridge Regression</span>
    <span class="n">ridge</span> <span class="o">=</span> <span class="n">RidgeCV</span><span class="p">(</span><span class="n">cv</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
    <span class="n">ridge</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_col_tr</span><span class="p">,</span> <span class="n">y_tr</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="s">&#39;Training R^2 for {} Ridge Regression: {}&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">title</span><span class="p">,</span> <span class="n">ridge</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_col_tr</span><span class="p">,</span> <span class="n">y_tr</span><span class="p">)))</span>
    <span class="k">print</span><span class="p">(</span><span class="s">&#39;Validation R^2 for {} Ridge Regression: {}&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">title</span><span class="p">,</span> <span class="n">ridge</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_col_val</span><span class="p">,</span> <span class="n">y_val</span><span class="p">)))</span>
    <span class="k">print</span><span class="p">()</span>
    <span class="c"># KNN, with max_neighbors found through CV</span>
    <span class="k">if</span> <span class="n">include_knn</span><span class="p">:</span>
        <span class="n">ks</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">50</span><span class="p">))</span>
        <span class="n">scores</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">scores_val</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">max_score</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">max_k</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">ks</span><span class="p">:</span>
            <span class="n">knn</span> <span class="o">=</span> <span class="n">KNeighborsRegressor</span><span class="p">(</span><span class="n">n_neighbors</span> <span class="o">=</span> <span class="n">k</span><span class="p">)</span>
            <span class="n">score</span> <span class="o">=</span> <span class="n">cross_val_score</span><span class="p">(</span><span class="n">knn</span><span class="p">,</span> <span class="n">X_col_tr</span><span class="p">,</span> <span class="n">y_tr</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
            <span class="n">scores</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">score</span><span class="p">)</span>
            <span class="n">scores_val</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">cross_val_score</span><span class="p">(</span><span class="n">knn</span><span class="p">,</span> <span class="n">X_col_val</span><span class="p">,</span> <span class="n">y_val</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">())</span>
            <span class="k">if</span> <span class="n">score</span> <span class="o">&gt;</span> <span class="n">max_score</span><span class="p">:</span>
                <span class="n">max_k</span> <span class="o">=</span> <span class="n">k</span>
                <span class="n">max_score</span> <span class="o">=</span> <span class="n">score</span>
        <span class="n">knn</span> <span class="o">=</span> <span class="n">KNeighborsRegressor</span><span class="p">(</span><span class="n">n_neighbors</span> <span class="o">=</span> <span class="n">max_k</span><span class="p">)</span>
        <span class="n">knn</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_col_tr</span><span class="p">,</span> <span class="n">y_tr</span><span class="p">)</span>
        <span class="k">print</span><span class="p">(</span><span class="s">&quot;Training Score for KNN (k={}) Model: {}&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">max_k</span><span class="p">,</span> <span class="n">knn</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_col_tr</span><span class="p">,</span> <span class="n">y_tr</span><span class="p">)))</span>
        <span class="k">print</span><span class="p">(</span><span class="s">&quot;Validation Score for KNN (k={}) Model: {}&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">max_k</span><span class="p">,</span> <span class="n">knn</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_col_val</span><span class="p">,</span> <span class="n">y_val</span><span class="p">)))</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="mi">10</span><span class="p">))</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">ks</span><span class="p">,</span> <span class="n">scores</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s">&#39;darkorange&#39;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">&#39;Train CV Scores&#39;</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">ks</span><span class="p">,</span> <span class="n">scores_val</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s">&#39;blue&#39;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">&#39;Validation CV Scores&#39;</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">max_k</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s">&#39;red&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">&#39;best k = {}&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">max_k</span><span class="p">),</span> <span class="n">linestyle</span><span class="o">=</span><span class="s">&#39;--&#39;</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">&#39;k from K-Neighbors model&#39;</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">&#39;CV Score&#39;</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s">&#39;Train, Validation, and Test CV Scores for KNN&#39;</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s">&quot;lower right&quot;</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">knn</span><span class="o">=</span><span class="bp">False</span>
    <span class="k">return</span> <span class="p">[</span><span class="n">lr</span><span class="p">,</span> <span class="n">lasso</span><span class="p">,</span> <span class="n">ridge</span><span class="p">,</span> <span class="n">knn</span><span class="p">]</span></code></pre></div>
<div class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">basic_cols</span> <span class="o">=</span> <span class="p">[</span><span class="s">&#39;median_age&#39;</span><span class="p">,</span> <span class="s">&#39;median_income&#39;</span><span class="p">,</span> <span class="s">&#39;male&#39;</span><span class="p">,</span> <span class="s">&#39;poverty&#39;</span><span class="p">,</span> <span class="s">&#39;population&#39;</span><span class="p">]</span>
<span class="n">basic_models</span> <span class="o">=</span> <span class="n">train_and_validate</span><span class="p">(</span><span class="n">X_tr</span><span class="p">,</span> <span class="n">X_val</span><span class="p">,</span> <span class="n">y_tr</span><span class="p">,</span> <span class="n">y_val</span><span class="p">,</span> <span class="n">basic_cols</span><span class="p">,</span> <span class="s">&quot;Basic&quot;</span><span class="p">,</span> <span class="n">add_const</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">poly</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span></code></pre></div>
<pre><code>Training R^2 for Basic Linear Regression: 0.10336462166830496  
Validation R^2 for Basic Linear Regression: 0.0905740112706801  
Training R^2 for Basic Lasso Regression: 0.10196365776150063  
Validation R^2 for Basic Lasso Regression: 0.08412300206289403  
Training R^2 for Basic Ridge Regression: 0.1031256353613198  
Validation R^2 for Basic Ridge Regression: 0.09070882715172046  
Training Score for KNN (k=13) Model: 0.384829110871148  
Validation Score for KNN (k=13) Model: 0.02369172929875007
</code></pre>
<p><img src="Prediction_model_files/Prediction_model_16_1.png" alt="png" /></p>
<h3 id="include-more-comprehensive-income-levels-in-each-model">3.1.4 Include more comprehensive income levels in each model</h3>
<p>To improve upon our previous model, we decided to incorporate the various breakdown of income composition: ‘$1 to $9,999 or loss’, ‘$10,000 to $14,999’, ‘$15,000 to $24,999’, ‘$25,000 to $34,999’, ‘$35,000 to $49,999’, ‘$50,000 to $64,999’, ‘$65,000 to $74,999’, and ‘$75,000 or more’ variables were included. As expected, the training and validation R-squared values all improved. For all three linear models of simple least squares, Lasso, and Ridge, the training and validation scores hovered at 0.14. The largest performance improvements really showed for the k-NN model for which k of 10 yielded a training score of 0.39 and validation score of 0.13. Albeit the inclusion of income levels did improve our models’ performances, there wasn’t a single model we deemed adequate, and we believe models could be further improved by incorporating all of the selected independent variables.</p>
<div class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">income_cols</span> <span class="o">=</span> <span class="n">basic_cols</span> <span class="o">+</span> <span class="p">[</span><span class="s">&#39;$1 to $9,999 or loss&#39;</span><span class="p">,</span> <span class="s">&#39;$10,000 to $14,999&#39;</span><span class="p">,</span> <span class="s">&#39;$15,000 to $24,999&#39;</span><span class="p">,</span>
               <span class="s">&#39;$25,000 to $34,999&#39;</span><span class="p">,</span> <span class="s">&#39;$35,000 to $49,999&#39;</span><span class="p">,</span> <span class="s">&#39;$50,000 to $64,999&#39;</span><span class="p">,</span>
               <span class="s">&#39;$65,000 to $74,999&#39;</span><span class="p">,</span> <span class="s">&#39;$75,000 or more&#39;</span><span class="p">]</span>
<span class="n">comp_income_models</span> <span class="o">=</span> <span class="n">train_and_validate</span><span class="p">(</span><span class="n">X_tr</span><span class="p">,</span> <span class="n">X_val</span><span class="p">,</span> <span class="n">y_tr</span><span class="p">,</span> <span class="n">y_val</span><span class="p">,</span> <span class="n">income_cols</span><span class="p">,</span> <span class="s">&quot;Income&quot;</span><span class="p">,</span> <span class="n">add_const</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">poly</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span></code></pre></div>
<pre><code>Training R^2 for Income Linear Regression: 0.14258477634600875  
Validation R^2 for Income Linear Regression: 0.1448900418121657  
Training R^2 for Income Lasso Regression: 0.1418296759572274  
Validation R^2 for Income Lasso Regression: 0.14534604419731512  
Training R^2 for Income Ridge Regression: 0.14155031635922333  
Validation R^2 for Income Ridge Regression: 0.14566808055357683  
Training Score for KNN (k=10) Model: 0.39124066387943257  
Validation Score for KNN (k=10) Model: 0.1320391142119982  
</code></pre>
<p><img src="Prediction_model_files/Prediction_model_18_1.png" alt="png" /></p>
<h4 id="include-all-columns-in-each-model">3.1.5 Include all columns in each model</h4>
<p>Before we delve into subset selection of our predictor variables, we ran our models to train on all columns: the aforementioned columns for the basic model; the added income breakdown predictors from above; racial composition of White, Black/African American, and Hispanic/Latino demographic; three education attainment breakdowns of less than high school graduate, high school graduate and equivalence, and some college or higher; foreign born/non-native population percentage of population; and the year variable. As for the reasons for not including other races such as Asian demographic, we mentioned in our EDA that this was intentional to reduce collinearity. Also, to address issues of multicollinearity in education attainment, we decided to sum some college or higher attainment of education into one predictor variable.</p>
<p>As expected, the three linear models all obtained improved training scores of 0.45 and validation scores of 0.41. The k-NN model with k of 18 beat all other models, posting the highest train R-squared of 0.51 and validation R-squared of 0.44. Now, we will look into both filtering the significant variables in linear regression and applying a stepwise predictor selection process for subset selection of the training set.</p>
<div class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">all_col_models</span> <span class="o">=</span> <span class="n">train_and_validate</span><span class="p">(</span><span class="n">X_tr</span><span class="p">,</span> <span class="n">X_val</span><span class="p">,</span> <span class="n">y_tr</span><span class="p">,</span> <span class="n">y_val</span><span class="p">,</span> <span class="n">X_tr</span><span class="o">.</span><span class="n">columns</span><span class="p">,</span> <span class="s">&quot;All Columns&quot;</span><span class="p">,</span> <span class="n">add_const</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">poly</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span></code></pre></div>
<pre><code>Training R^2 for All Columns Linear Regression: 0.4462319942491597  
Validation R^2 for All Columns Linear Regression: 0.4108502357189554  
Training R^2 for All Columns Lasso Regression: 0.44560209039581394  
Validation R^2 for All Columns Lasso Regression: 0.4102549436785793  
Training R^2 for All Columns Ridge Regression: 0.4458059433121516  
Validation R^2 for All Columns Ridge Regression: 0.41016795215679747  
Training Score for KNN (k=18) Model: 0.5069220726159709  
Validation Score for KNN (k=18) Model: 0.44440789682622034  
</code></pre>
<p><img src="Prediction_model_files/Prediction_model_21_1.png" alt="png" /></p>
<h3 id="subset-selection">3.2 Subset Selection</h3>
<h4 id="ols-model-to-determine-statistically-significant-predictors">3.2.1 OLS Model to determine statistically significant predictors</h4>
<p>First, we re-ran the linear regression with all predictor columns using the OLS Model which returned the same train and validation scores as 3.1.5 Model of 0.45 and 0.41, respectively. At alpha of 0.05, the eight statistically significant variables are as follows: the constant/slope, Black/African American demographic, highschool or equivalent attainment, median age, total population, less than high school demographic, and White demographic predictors. You can see the respective coefficients for each predictor in the summary table of the result; but it is, in fact, more informative to see other non-significant predictor variables that could bring more insight: income variables ‘$25,000 to $34,999’, ‘$65,000 to $74,999’,  and ‘ $75,000 or more’ variables with p-values of  0.07, 0.07, and 0.12, respective; ‘college’ education variable with p-value of 0.05; ‘median_income’ variable with p-value of 0.11; and ‘year’ variable with p-value of 0.521.</p>
<p>However, after examining that the coefficients of those variables, the income breakdown predictors show that their overall impact on the crime rates aren’t significant and that the granularity of breakdown eliminates their significance with multicollinearity. We do believe that the college, median_income, and year variables are very interesting predictors to consider; in fact, we imagine one of these could be selected in a stepwise predictor selection process in subsequent enhancements. For now, we will proceed to rerun our train_and_validate function by only using the eight significant variables.</p>
<div class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">X_all_train</span> <span class="o">=</span> <span class="n">X_tr</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">pd</span><span class="o">.</span><span class="n">to_numeric</span><span class="p">,</span> <span class="n">errors</span><span class="o">=</span><span class="s">&#39;coerce&#39;</span><span class="p">)</span>
<span class="n">X_all_val</span> <span class="o">=</span> <span class="n">X_val</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">pd</span><span class="o">.</span><span class="n">to_numeric</span><span class="p">,</span> <span class="n">errors</span><span class="o">=</span><span class="s">&#39;coerce&#39;</span><span class="p">)</span>
<span class="n">imputer</span> <span class="o">=</span> <span class="n">Imputer</span><span class="p">(</span><span class="n">strategy</span><span class="o">=</span><span class="s">&#39;mean&#39;</span><span class="p">)</span>
<span class="n">X_const_train</span> <span class="o">=</span> <span class="n">imputer</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X_all_train</span><span class="p">)</span>
<span class="n">X_const_val</span> <span class="o">=</span> <span class="n">imputer</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X_all_val</span><span class="p">)</span>
<span class="n">scaler</span> <span class="o">=</span> <span class="n">MinMaxScaler</span><span class="p">()</span>
<span class="n">X_const_train</span> <span class="o">=</span> <span class="n">sm</span><span class="o">.</span><span class="n">add_constant</span><span class="p">(</span><span class="n">scaler</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X_const_train</span><span class="p">))</span>
<span class="n">X_const_val</span> <span class="o">=</span> <span class="n">sm</span><span class="o">.</span><span class="n">add_constant</span><span class="p">(</span><span class="n">scaler</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X_const_val</span><span class="p">))</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">OLS</span><span class="p">(</span><span class="n">endog</span><span class="o">=</span><span class="n">y_tr</span><span class="p">,</span> <span class="n">exog</span><span class="o">=</span><span class="n">X_const_train</span><span class="p">)</span>
<span class="n">result</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
<span class="n">y_hat_train</span> <span class="o">=</span> <span class="n">result</span><span class="o">.</span><span class="n">predict</span><span class="p">()</span>
<span class="n">y_hat_val</span> <span class="o">=</span> <span class="n">result</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">exog</span><span class="o">=</span><span class="n">X_const_val</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">&#39;Train R^2 =&#39;</span><span class="p">,</span> <span class="n">r2_score</span><span class="p">(</span><span class="n">y_tr</span><span class="p">,</span> <span class="n">y_hat_train</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="s">&#39;Validation R^2 =&#39;</span><span class="p">,</span> <span class="n">r2_score</span><span class="p">(</span><span class="n">y_val</span><span class="p">,</span> <span class="n">y_hat_val</span><span class="p">))</span></code></pre></div>
<pre><code>Train R^2 = 0.446231994249  
Validation R^2 = 0.410850235719  
</code></pre>
<div class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">xlabs</span> <span class="o">=</span> <span class="p">[</span><span class="s">&#39;const&#39;</span><span class="p">]</span> <span class="o">+</span> <span class="nb">list</span><span class="p">(</span><span class="n">X_train</span><span class="o">.</span><span class="n">columns</span><span class="p">)</span>
<span class="n">result</span><span class="o">.</span><span class="n">summary</span><span class="p">(</span><span class="n">xname</span><span class="o">=</span><span class="n">xlabs</span><span class="p">)</span></code></pre></div>
<table class="simpletable">
<caption>OLS Regression Results</caption>
<tr>
  <th>Dep. Variable:</th>    <td>murder_per_100000</td> <th>  R-squared:         </th> <td>   0.446</td>
</tr>
<tr>
  <th>Model:</th>                   <td>OLS</td>        <th>  Adj. R-squared:    </th> <td>   0.441</td>
</tr>
<tr>
  <th>Method:</th>             <td>Least Squares</td>   <th>  F-statistic:       </th> <td>   79.70</td>
</tr>
<tr>
  <th>Date:</th>             <td>Wed, 06 Dec 2017</td>  <th>  Prob (F-statistic):</th> <td>5.55e-248</td>
</tr>
<tr>
  <th>Time:</th>                 <td>20:15:43</td>      <th>  Log-Likelihood:    </th> <td> -4719.9</td>
</tr>
<tr>
  <th>No. Observations:</th>      <td>  2099</td>       <th>  AIC:               </th> <td>   9484.</td>
</tr>
<tr>
  <th>Df Residuals:</th>          <td>  2077</td>       <th>  BIC:               </th> <td>   9608.</td>
</tr>
<tr>
  <th>Df Model:</th>              <td>    21</td>       <th>                     </th>     <td> </td>    
</tr>
<tr>
  <th>Covariance Type:</th>      <td>nonrobust</td>     <th>                     </th>     <td> </td>    
</tr>
</table>
<table class="simpletable">
<tr>
            <td></td>              <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P&gt;|t|</th>  <th>[0.025</th>    <th>0.975]</th>  
</tr>
<tr>
  <th>const</th>                <td>    5.7329</td> <td>    1.957</td> <td>    2.930</td> <td> 0.003</td> <td>    1.896</td> <td>    9.570</td>
</tr>
<tr>
  <th>$1 to $9,999 or loss</th> <td>    0.7105</td> <td>    0.793</td> <td>    0.896</td> <td> 0.370</td> <td>   -0.845</td> <td>    2.266</td>
</tr>
<tr>
  <th>$10,000 to $14,999</th>   <td>    0.5470</td> <td>    0.596</td> <td>    0.917</td> <td> 0.359</td> <td>   -0.622</td> <td>    1.716</td>
</tr>
<tr>
  <th>$15,000 to $24,999</th>   <td>   -0.5599</td> <td>    0.906</td> <td>   -0.618</td> <td> 0.537</td> <td>   -2.337</td> <td>    1.217</td>
</tr>
<tr>
  <th>$25,000 to $34,999</th>   <td>    1.1013</td> <td>    0.616</td> <td>    1.787</td> <td> 0.074</td> <td>   -0.108</td> <td>    2.310</td>
</tr>
<tr>
  <th>$35,000 to $49,999</th>   <td>    0.5251</td> <td>    0.673</td> <td>    0.780</td> <td> 0.435</td> <td>   -0.794</td> <td>    1.845</td>
</tr>
<tr>
  <th>$50,000 to $64,999</th>   <td>   -0.9803</td> <td>    0.726</td> <td>   -1.350</td> <td> 0.177</td> <td>   -2.404</td> <td>    0.444</td>
</tr>
<tr>
  <th>$65,000 to $74,999</th>   <td>    1.1605</td> <td>    0.642</td> <td>    1.809</td> <td> 0.071</td> <td>   -0.098</td> <td>    2.419</td>
</tr>
<tr>
  <th>$75,000 or more</th>      <td>    1.6142</td> <td>    1.042</td> <td>    1.549</td> <td> 0.122</td> <td>   -0.430</td> <td>    3.658</td>
</tr>
<tr>
  <th>black</th>                <td>    3.7697</td> <td>    0.489</td> <td>    7.710</td> <td> 0.000</td> <td>    2.811</td> <td>    4.729</td>
</tr>
<tr>
  <th>college</th>              <td>    1.6800</td> <td>    0.868</td> <td>    1.936</td> <td> 0.053</td> <td>   -0.022</td> <td>    3.382</td>
</tr>
<tr>
  <th>foreign</th>              <td>   -1.7076</td> <td>    0.472</td> <td>   -3.616</td> <td> 0.000</td> <td>   -2.634</td> <td>   -0.782</td>
</tr>
<tr>
  <th>highschool</th>           <td>    3.2544</td> <td>    0.784</td> <td>    4.153</td> <td> 0.000</td> <td>    1.718</td> <td>    4.791</td>
</tr>
<tr>
  <th>hispanic</th>             <td>    0.8060</td> <td>    0.597</td> <td>    1.349</td> <td> 0.177</td> <td>   -0.365</td> <td>    1.977</td>
</tr>
<tr>
  <th>male</th>                 <td>   -0.6007</td> <td>    0.672</td> <td>   -0.893</td> <td> 0.372</td> <td>   -1.919</td> <td>    0.718</td>
</tr>
<tr>
  <th>median_age</th>           <td>    1.7845</td> <td>    0.705</td> <td>    2.532</td> <td> 0.011</td> <td>    0.402</td> <td>    3.167</td>
</tr>
<tr>
  <th>median_income</th>        <td>   -1.8899</td> <td>    1.171</td> <td>   -1.614</td> <td> 0.107</td> <td>   -4.187</td> <td>    0.407</td>
</tr>
<tr>
  <th>population</th>           <td>    1.7170</td> <td>    0.820</td> <td>    2.094</td> <td> 0.036</td> <td>    0.109</td> <td>    3.325</td>
</tr>
<tr>
  <th>poverty</th>              <td>    0.0808</td> <td>    0.700</td> <td>    0.115</td> <td> 0.908</td> <td>   -1.293</td> <td>    1.454</td>
</tr>
<tr>
  <th>uneducated</th>           <td>    3.2330</td> <td>    0.912</td> <td>    3.544</td> <td> 0.000</td> <td>    1.444</td> <td>    5.022</td>
</tr>
<tr>
  <th>white</th>                <td>   -8.7646</td> <td>    0.625</td> <td>  -14.016</td> <td> 0.000</td> <td>   -9.991</td> <td>   -7.538</td>
</tr>
<tr>
  <th>year</th>                 <td>   -0.1063</td> <td>    0.166</td> <td>   -0.641</td> <td> 0.521</td> <td>   -0.431</td> <td>    0.219</td>
</tr>
</table>
<table class="simpletable">
<tr>
  <th>Omnibus:</th>       <td>521.600</td> <th>  Durbin-Watson:     </th> <td>   2.058</td>
</tr>
<tr>
  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td>2342.335</td>
</tr>
<tr>
  <th>Skew:</th>          <td> 1.121</td>  <th>  Prob(JB):          </th> <td>    0.00</td>
</tr>
<tr>
  <th>Kurtosis:</th>      <td> 7.665</td>  <th>  Cond. No.          </th> <td>    102.</td>
</tr>
</table>
<div class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">most_significant</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">xlabs</span><span class="p">)[</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">result</span><span class="o">.</span><span class="n">pvalues</span> <span class="o">&lt;</span> <span class="mf">0.05</span><span class="p">)][</span><span class="mi">1</span><span class="p">:]</span></code></pre></div>
<h4 id="train-models-on-most-significant-predictors-determined-by-ols">3.2.2 Train models on most significant predictors determined by OLS</h4>
<p>Selecting the statistically significant variables from the least squares linear regression in 3.5.1 at alpha of 0.05, we obtain the same performance results, again, for the three linear regression models as we did above. However, the most surprising improvements came from the k-NN model with best cross-validated k of 7 that had training R-squared value of 0.66 and validation score of 0.50. The k-NN, again, beats its own record solely using seven predictor variables and a constant.</p>
<div class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">OLS_sig_col_models</span> <span class="o">=</span> <span class="n">train_and_validate</span><span class="p">(</span><span class="n">X_tr</span><span class="p">,</span> <span class="n">X_val</span><span class="p">,</span> <span class="n">y_tr</span><span class="p">,</span> <span class="n">y_val</span><span class="p">,</span> <span class="n">most_significant</span><span class="p">,</span> <span class="s">&quot;Most Significant OLS Predictors&quot;</span><span class="p">,</span> <span class="n">add_const</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">poly</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span></code></pre></div>
<pre><code>Training R^2 for Most Significant OLS Predictors Linear Regression: 0.4403589258248584  
Validation R^2 for Most Significant OLS Predictors Linear Regression: 0.40655554960022205  
Training R^2 for Most Significant OLS Predictors Lasso Regression: 0.4403526474899125  
Validation R^2 for Most Significant OLS Predictors Lasso Regression: 0.40658558175226805  
Training R^2 for Most Significant OLS Predictors Ridge Regression: 0.4402215161510081  
Validation R^2 for Most Significant OLS Predictors Ridge Regression: 0.406473259967734  
Training Score for KNN (k=7) Model: 0.6573569971226408  
Validation Score for KNN (k=7) Model: 0.4990922977081862  
</code></pre>
<p><img src="Prediction_model_files/Prediction_model_29_1.png" alt="png" /></p>
<h4 id="stepwise-bic-subset-selection">3.2.3 Stepwise BIC Subset Selection</h4>
<p>We utilized both forwards and backwards stepwise selection processes based on the BIC scores of linear regressions. Both the forwards and backwards stepwise selection processes returned the identical predictors: ‘uneducated’  and ‘highschool’ education variables; ‘black’, ‘white’, and ‘foreign’ demographic variables; and ‘’median_age’ variable.</p>
<div class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">def</span> <span class="nf">step_forwards_backwards</span><span class="p">(</span><span class="n">df</span><span class="p">,</span> <span class="n">y_vars</span><span class="p">,</span> <span class="n">direction</span><span class="o">=</span><span class="s">&#39;forward&#39;</span><span class="p">):</span>
    <span class="k">assert</span> <span class="n">direction</span> <span class="ow">in</span> <span class="p">[</span><span class="s">&#39;forward&#39;</span><span class="p">,</span> <span class="s">&#39;backward&#39;</span><span class="p">]</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">y_vars</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">predictors</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">columns</span><span class="p">)</span>
    <span class="n">selected_predictors</span> <span class="o">=</span> <span class="nb">set</span><span class="p">()</span> <span class="k">if</span> <span class="n">direction</span><span class="o">==</span><span class="s">&#39;forward&#39;</span> <span class="k">else</span> <span class="nb">set</span><span class="p">(</span><span class="n">predictors</span><span class="p">)</span>
    <span class="n">n</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">best_bic</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">inf</span>
    <span class="n">best_bics</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">best_models</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">imputer</span> <span class="o">=</span> <span class="n">Imputer</span><span class="p">(</span><span class="n">strategy</span><span class="o">=</span><span class="s">&#39;mean&#39;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">direction</span> <span class="o">==</span> <span class="s">&#39;forward&#39;</span><span class="p">:</span>
        <span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">n</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">([</span><span class="n">X</span><span class="p">,</span> <span class="n">df</span><span class="p">[</span><span class="nb">list</span><span class="p">(</span><span class="n">selected_predictors</span><span class="p">)]</span><span class="o">.</span><span class="n">values</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">while</span> <span class="p">(</span><span class="bp">True</span><span class="p">):</span>
            <span class="n">possible_bic_scores</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="n">possible_predictors</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">selected_predictors</span> <span class="o">^</span> <span class="n">predictors</span><span class="p">)</span>
            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">possible_predictors</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="k">break</span>
            <span class="k">for</span> <span class="n">predictor</span> <span class="ow">in</span> <span class="n">possible_predictors</span><span class="p">:</span>
                <span class="n">x_temp</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">([</span><span class="n">X</span><span class="p">,</span> <span class="n">df</span><span class="p">[</span><span class="n">predictor</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">)],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
                <span class="n">x_temp</span> <span class="o">=</span> <span class="n">imputer</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">x_temp</span><span class="p">)</span>
                <span class="n">model</span> <span class="o">=</span> <span class="n">OLS</span><span class="p">(</span><span class="n">endog</span><span class="o">=</span><span class="n">y</span><span class="p">,</span> <span class="n">exog</span><span class="o">=</span><span class="n">x_temp</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
                <span class="n">bic</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">bic</span>
                <span class="n">possible_bic_scores</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">bic</span><span class="p">)</span>
            <span class="n">best_predictor_ix</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmin</span><span class="p">(</span><span class="n">possible_bic_scores</span><span class="p">)</span>
            <span class="n">best_predictor</span> <span class="o">=</span> <span class="n">possible_predictors</span><span class="p">[</span><span class="n">best_predictor_ix</span><span class="p">]</span>
            <span class="n">best_bic</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">possible_bic_scores</span><span class="p">)</span>
            <span class="n">best_bics</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">best_bic</span><span class="p">)</span>
            <span class="n">selected_predictors</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">best_predictor</span><span class="p">)</span>            
            <span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">([</span><span class="n">X</span><span class="p">,</span> <span class="n">df</span><span class="p">[</span><span class="n">best_predictor</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">)],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">best_models</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">selected_predictors</span><span class="p">))</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">while</span> <span class="p">(</span><span class="bp">True</span><span class="p">):</span>
            <span class="n">possible_bic_scores</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="n">possible_predictors</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">selected_predictors</span><span class="p">)</span>
            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">possible_predictors</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="k">break</span>
            <span class="k">for</span> <span class="n">predictor</span> <span class="ow">in</span> <span class="n">possible_predictors</span><span class="p">:</span>
                <span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">([</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">n</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">),</span> <span class="n">df</span><span class="p">[</span><span class="nb">list</span><span class="p">(</span><span class="n">selected_predictors</span> <span class="o">-</span> <span class="nb">set</span><span class="p">([</span><span class="n">predictor</span><span class="p">]))]</span><span class="o">.</span><span class="n">values</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
                <span class="n">X</span> <span class="o">=</span> <span class="n">imputer</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
                <span class="n">model</span> <span class="o">=</span> <span class="n">OLS</span><span class="p">(</span><span class="n">endog</span><span class="o">=</span><span class="n">y</span><span class="p">,</span> <span class="n">exog</span><span class="o">=</span><span class="n">X</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
                <span class="n">bic</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">bic</span>
                <span class="n">possible_bic_scores</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">bic</span><span class="p">)</span>
            <span class="n">best_predictor_ix</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmin</span><span class="p">(</span><span class="n">possible_bic_scores</span><span class="p">)</span>
            <span class="n">best_predictor</span> <span class="o">=</span> <span class="n">possible_predictors</span><span class="p">[</span><span class="n">best_predictor_ix</span><span class="p">]</span>
            <span class="n">best_bic</span> <span class="o">=</span> <span class="n">possible_bic_scores</span><span class="p">[</span><span class="n">best_predictor_ix</span><span class="p">]</span>
            <span class="n">selected_predictors</span><span class="o">.</span><span class="n">discard</span><span class="p">(</span><span class="n">best_predictor</span><span class="p">)</span>
            <span class="n">best_bics</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">best_bic</span><span class="p">)</span>
            <span class="n">best_models</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">selected_predictors</span><span class="p">))</span>
    <span class="n">index_of_best_bic</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmin</span><span class="p">(</span><span class="n">best_bics</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">best_models</span><span class="p">[</span><span class="n">index_of_best_bic</span><span class="p">]</span></code></pre></div>
<div class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">predictors_forward</span> <span class="o">=</span> <span class="n">step_forwards_backwards</span><span class="p">(</span><span class="n">X_all_train</span><span class="p">,</span> <span class="n">y_tr</span><span class="p">,</span> <span class="n">direction</span><span class="o">=</span><span class="s">&#39;forward&#39;</span><span class="p">)</span>
<span class="n">predictors_forward</span></code></pre></div>
<pre><code>['highschool', 'black', 'median_age', 'foreign', 'uneducated', 'white']
</code></pre>
<div class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">predictors_backward</span> <span class="o">=</span> <span class="n">step_forwards_backwards</span><span class="p">(</span><span class="n">X_all_train</span><span class="p">,</span> <span class="n">y_tr</span><span class="p">,</span> <span class="n">direction</span><span class="o">=</span><span class="s">&#39;backward&#39;</span><span class="p">)</span>
<span class="n">predictors_backward</span></code></pre></div>
<pre><code>['median_age', 'white', 'highschool', 'black', 'foreign', 'uneducated']
</code></pre>
<h4 id="use-columns-from-forwards-and-backwards-selection-in-ridge-lasso-k-nn-regression">3.2.4 Use columns from forwards and backwards selection in Ridge, Lasso, K-NN Regression</h4>
<p>The result from training a new model based on the stepwise selected columns resulted in similar results as the model in 3.2.2. This is not too surprising as many of these same predictor variables were selected under statistically significant method above. At the moment, our next method is to incorporate interaction variables in polynomial regression using PolynomialFeatures.</p>
<div class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">fwd_models</span> <span class="o">=</span> <span class="n">train_and_validate</span><span class="p">(</span><span class="n">X_tr</span><span class="p">,</span> <span class="n">X_val</span><span class="p">,</span> <span class="n">y_tr</span><span class="p">,</span> <span class="n">y_val</span><span class="p">,</span> <span class="n">predictors_forward</span><span class="p">,</span> <span class="s">&quot;Forward BIC Selection&quot;</span><span class="p">,</span> <span class="n">add_const</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">poly</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span></code></pre></div>
<pre><code>Training R^2 for Forward BIC Selection Linear Regression: 0.4395767505680862  
Validation R^2 for Forward BIC Selection Linear Regression: 0.4058955039695988  
Training R^2 for Forward BIC Selection Lasso Regression: 0.4395728491071222  
Validation R^2 for Forward BIC Selection Lasso Regression: 0.40591249707502186  
Training R^2 for Forward BIC Selection Ridge Regression: 0.4394289184448102  
Validation R^2 for Forward BIC Selection Ridge Regression: 0.40572943969483877  
Training Score for KNN (k=7) Model: 0.6556020765509312  
Validation Score for KNN (k=7) Model: 0.49629440076890985  
</code></pre>
<p><img src="Prediction_model_files/Prediction_model_35_1.png" alt="png" /></p>
<div class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">back_models</span> <span class="o">=</span> <span class="n">train_and_validate</span><span class="p">(</span><span class="n">X_tr</span><span class="p">,</span> <span class="n">X_val</span><span class="p">,</span> <span class="n">y_tr</span><span class="p">,</span> <span class="n">y_val</span><span class="p">,</span> <span class="n">predictors_backward</span><span class="p">,</span> <span class="s">&quot;Backward BIC Selection&quot;</span><span class="p">,</span> <span class="n">add_const</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">poly</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span></code></pre></div>
<pre><code>Training R^2 for Backward BIC Selection Linear Regression: 0.4395767505680862  
Validation R^2 for Backward BIC Selection Linear Regression: 0.4058955039695988  
Training R^2 for Backward BIC Selection Lasso Regression: 0.4395729554888817  
Validation R^2 for Backward BIC Selection Lasso Regression: 0.4059089097399196  
Training R^2 for Backward BIC Selection Ridge Regression: 0.4394289184448102  
Validation R^2 for Backward BIC Selection Ridge Regression: 0.40572943969483877  
Training Score for KNN (k=7) Model: 0.6556020765509312  
Validation Score for KNN (k=7) Model: 0.49629440076890985  
</code></pre>
<p><img src="Prediction_model_files/Prediction_model_36_1.png" alt="png" /></p>
<div class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">union</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="n">predictors_forward</span><span class="p">)</span><span class="o">.</span><span class="n">union</span><span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="n">predictors_backward</span><span class="p">)))</span>
<span class="n">union_models</span> <span class="o">=</span> <span class="n">train_and_validate</span><span class="p">(</span><span class="n">X_tr</span><span class="p">,</span> <span class="n">X_val</span><span class="p">,</span> <span class="n">y_tr</span><span class="p">,</span> <span class="n">y_val</span><span class="p">,</span> <span class="n">union</span><span class="p">,</span> <span class="s">&quot;Union of BIC Selected Columns&quot;</span><span class="p">,</span> <span class="n">add_const</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">poly</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span></code></pre></div>
<pre><code>Training R^2 for Union of BIC Selected Columns Linear Regression: 0.4395767505680862  
Validation R^2 for Union of BIC Selected Columns Linear Regression: 0.4058955039695988  
Training R^2 for Union of BIC Selected Columns Lasso Regression: 0.4395728491071222  
Validation R^2 for Union of BIC Selected Columns Lasso Regression: 0.40591249707502186  
Training R^2 for Union of BIC Selected Columns Ridge Regression: 0.4394289184448102  
Validation R^2 for Union of BIC Selected Columns Ridge Regression: 0.40572943969483877  
Training Score for KNN (k=7) Model: 0.6556020765509312  
Validation Score for KNN (k=7) Model: 0.49629440076890985  
</code></pre>
<p><img src="Prediction_model_files/Prediction_model_37_1.png" alt="png" /></p>
<div class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">intersection</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="n">predictors_forward</span><span class="p">)</span><span class="o">.</span><span class="n">intersection</span><span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="n">predictors_backward</span><span class="p">)))</span>
<span class="n">inter_models</span> <span class="o">=</span> <span class="n">train_and_validate</span><span class="p">(</span><span class="n">X_tr</span><span class="p">,</span> <span class="n">X_val</span><span class="p">,</span> <span class="n">y_tr</span><span class="p">,</span> <span class="n">y_val</span><span class="p">,</span> <span class="n">intersection</span><span class="p">,</span> <span class="s">&quot;Intersection of BIC Selected Columns&quot;</span><span class="p">,</span> <span class="n">add_const</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">poly</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span></code></pre></div>
<pre><code>Training R^2 for Intersection of BIC Selected Columns Linear Regression: 0.4395767505680862  
Validation R^2 for Intersection of BIC Selected Columns Linear Regression: 0.4058955039695988  
Training R^2 for Intersection of BIC Selected Columns Lasso Regression: 0.4395728491071222  
Validation R^2 for Intersection of BIC Selected Columns Lasso Regression: 0.40591249707502186  
Training R^2 for Intersection of BIC Selected Columns Ridge Regression: 0.4394289184448102  
Validation R^2 for Intersection of BIC Selected Columns Ridge Regression: 0.40572943969483877  
Training Score for KNN (k=7) Model: 0.6556020765509312  
Validation Score for KNN (k=7) Model: 0.49629440076890985  
</code></pre>
<p><img src="Prediction_model_files/Prediction_model_38_1.png" alt="png" /></p>
<h4 id="polynomial-features-and-interaction-variables">3.2.5 Polynomial Features and Interaction Variables</h4>
<p>We transformed the numeric variables into fourth degree polynomial with interaction for the selected columns in our subselection methodology in 3.2.4 to run a multinomial regression model. Since we are creating many predictor columns, we ran into problems of overfitting with the k-NN model, obtaining very extremely high training scores and negative validation scores. Therefore, we only ran the three regression methods of polynomial, Lasso with cross-validation, and Ridge with cross-validation models. The results were surprising. First, the polynomial variables on linear regression method returned signs of overfitting with high train score of 0.59 and negative validation score of -2.67. Second, the Lasso and Ridge models returned slightly improved train/validation scores of 0.49/0.43 and 0.50/0.42, respectively. Still, the k-NN model from 3.6 outperforms all of its competition. The best trajectory to enhance our model might be to add a new dimension of data.</p>
<div class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">union</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="n">predictors_forward</span><span class="p">)</span><span class="o">.</span><span class="n">union</span><span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="n">predictors_backward</span><span class="p">)))</span>
<span class="n">union_poly_models</span> <span class="o">=</span> <span class="n">train_and_validate</span><span class="p">(</span><span class="n">X_tr</span><span class="p">,</span> <span class="n">X_val</span><span class="p">,</span> <span class="n">y_tr</span><span class="p">,</span> <span class="n">y_val</span><span class="p">,</span> <span class="n">union</span><span class="p">,</span>
                                       <span class="s">&quot;Union of BIC Selected Columns with Polynomial Variables&quot;</span><span class="p">,</span>
                                       <span class="n">add_const</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">poly</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">include_knn</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span></code></pre></div>
<pre><code>Training R^2 for Union of BIC Selected Columns with Polynomial Variables Linear Regression: 0.5925936931515721  
Validation R^2 for Union of BIC Selected Columns with Polynomial Variables Linear Regression: -2.67272217648836  
Training R^2 for Union of BIC Selected Columns with Polynomial Variables Lasso Regression: 0.48592701270872163  
Validation R^2 for Union of BIC Selected Columns with Polynomial Variables Lasso Regression: 0.42508113020444754  
Training R^2 for Union of BIC Selected Columns with Polynomial Variables Ridge Regression: 0.49781968947957717  
Validation R^2 for Union of BIC Selected Columns with Polynomial Variables Ridge Regression: 0.42074141255544384  
</code></pre>
<div class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">intersection</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="n">predictors_forward</span><span class="p">)</span><span class="o">.</span><span class="n">intersection</span><span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="n">predictors_backward</span><span class="p">)))</span>
<span class="n">inter_poly_models</span> <span class="o">=</span> <span class="n">train_and_validate</span><span class="p">(</span><span class="n">X_tr</span><span class="p">,</span> <span class="n">X_val</span><span class="p">,</span> <span class="n">y_tr</span><span class="p">,</span> <span class="n">y_val</span><span class="p">,</span> <span class="n">intersection</span><span class="p">,</span>
                                       <span class="s">&quot;Intersection of BIC Selected Columns with Polynomial Variables&quot;</span><span class="p">,</span>
                                       <span class="n">add_const</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">poly</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">include_knn</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span></code></pre></div>
<pre><code>Training R^2 for Intersection of BIC Selected Columns with Polynomial Variables Linear Regression: 0.5925936931515721  
Validation R^2 for Intersection of BIC Selected Columns with Polynomial Variables Linear Regression: -2.67272217648836  
Training R^2 for Intersection of BIC Selected Columns with Polynomial Variables Lasso Regression: 0.48592701270872163  
Validation R^2 for Intersection of BIC Selected Columns with Polynomial Variables Lasso Regression: 0.42508113020444754  
Training R^2 for Intersection of BIC Selected Columns with Polynomial Variables Ridge Regression: 0.49781968947957717  
Validation R^2 for Intersection of BIC Selected Columns with Polynomial Variables Ridge Regression: 0.42074141255544384  
</code></pre>
<h3 id="model-trained-with-atf-data">3.3 Model Trained with ATF Data</h3>
<h4 id="retrieve-atf-data-and-merge-dfs">3.3.1 Retrieve ATF Data and merge DFs</h4>
<p>As we mentioned in our related work review in 2.3, we wanted to incorporate firearms data into our predictor variables. By merging new ATF dataframes with existing dataframes composed of both FBI and Census data, we were able to bring in a proxy for gun ownership by having state-level data on illegal firearms that were sourced and recovered. Although we could not find any MSA-level data regarding gun ownership, we wanted to see how our models would perform with the two added variables: ‘firearms_soruced’ and ‘firearms_recovered’.</p>
<div class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">def</span> <span class="nf">get_atf_vals</span><span class="p">(</span><span class="n">states</span><span class="p">,</span> <span class="n">atf_df</span><span class="p">,</span> <span class="n">recover</span><span class="o">=</span><span class="bp">True</span><span class="p">):</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">states</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">state_counts</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">state</span> <span class="ow">in</span> <span class="n">states</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">recover</span><span class="p">:</span>
                <span class="n">count</span> <span class="o">=</span> <span class="n">atf_df</span><span class="p">[</span><span class="n">atf_df</span><span class="p">[</span><span class="s">&quot;State&quot;</span><span class="p">]</span> <span class="o">==</span> <span class="n">state</span><span class="p">][</span><span class="s">&#39;Total Recovered&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">count</span> <span class="o">=</span> <span class="n">atf_df</span><span class="p">[</span><span class="n">atf_df</span><span class="p">[</span><span class="s">&quot;State&quot;</span><span class="p">]</span> <span class="o">==</span> <span class="n">state</span><span class="p">][</span><span class="s">&#39;Total Sourced&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span>
            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">count</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                <span class="n">state_counts</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">count</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">state_counts</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">recover</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">atf_df</span><span class="p">[</span><span class="n">atf_df</span><span class="p">[</span><span class="s">&quot;State&quot;</span><span class="p">]</span> <span class="o">==</span> <span class="n">states</span><span class="p">[</span><span class="mi">0</span><span class="p">]][</span><span class="s">&#39;Total Recovered&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">atf_df</span><span class="p">[</span><span class="n">atf_df</span><span class="p">[</span><span class="s">&quot;State&quot;</span><span class="p">]</span> <span class="o">==</span> <span class="n">states</span><span class="p">[</span><span class="mi">0</span><span class="p">]][</span><span class="s">&#39;Total Sourced&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span></code></pre></div>
<div class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">atf_filepaths</span> <span class="o">=</span> <span class="p">[</span><span class="n">f</span> <span class="k">for</span> <span class="n">f</span> <span class="ow">in</span> <span class="n">listdir</span><span class="p">(</span><span class="s">&#39;atf_csvs&#39;</span><span class="p">)</span> <span class="k">if</span> <span class="n">isfile</span><span class="p">(</span><span class="n">join</span><span class="p">(</span><span class="s">&#39;atf_csvs&#39;</span><span class="p">,</span> <span class="n">f</span><span class="p">))]</span></code></pre></div>
<div class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">atf_dict</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">()</span>
<span class="k">for</span> <span class="nb">file</span> <span class="ow">in</span> <span class="n">atf_filepaths</span><span class="p">:</span>
    <span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="n">join</span><span class="p">(</span><span class="s">&#39;atf_csvs&#39;</span><span class="p">,</span> <span class="nb">file</span><span class="p">))</span>
    <span class="n">atf_dict</span><span class="p">[</span><span class="nb">file</span><span class="p">[:</span><span class="mi">4</span><span class="p">]]</span> <span class="o">=</span> <span class="n">df</span></code></pre></div>
<div class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">df_dict</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">()</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">11</span><span class="p">):</span>
    <span class="n">df_fbi</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="n">join</span><span class="p">(</span><span class="s">&#39;csvs/&#39;</span><span class="p">,</span> <span class="n">csv_filepaths</span><span class="p">[</span><span class="n">i</span><span class="p">]))</span>
    <span class="n">df_census</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="n">join</span><span class="p">(</span><span class="n">census_data_path</span><span class="p">,</span> <span class="n">census_filepaths</span><span class="p">[</span><span class="n">i</span><span class="p">]),</span> <span class="n">skiprows</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">drop</span><span class="p">([</span><span class="s">&#39;Id&#39;</span><span class="p">,</span> <span class="s">&#39;Id2&#39;</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="n">col</span> <span class="k">for</span> <span class="n">col</span> <span class="ow">in</span> <span class="n">df_census</span><span class="o">.</span><span class="n">columns</span> <span class="k">if</span> <span class="s">&#39;Native&#39;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">col</span> <span class="ow">and</span> <span class="s">&#39;Margin of Error&#39;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">col</span><span class="p">]</span>
    <span class="n">foreign</span> <span class="o">=</span> <span class="p">[</span><span class="n">x</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">df_census</span><span class="o">.</span><span class="n">columns</span> <span class="k">if</span> <span class="s">&#39;born outside&#39;</span> <span class="ow">in</span> <span class="n">x</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">non_native</span> <span class="o">=</span> <span class="n">df_census</span><span class="p">[</span><span class="n">foreign</span><span class="p">]</span>
    <span class="n">df_census</span> <span class="o">=</span> <span class="n">df_census</span><span class="p">[</span><span class="n">columns</span><span class="p">]</span>
    <span class="n">df_census</span><span class="p">[</span><span class="s">&#39;non_native&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">non_native</span>
    <span class="n">df</span> <span class="o">=</span> <span class="n">merge_df_msa</span><span class="p">(</span><span class="n">df_fbi</span><span class="p">,</span> <span class="n">df_census</span><span class="p">)</span>
    <span class="c"># Compute murders per 100,000 people</span>
    <span class="n">population</span> <span class="o">=</span> <span class="p">[</span><span class="n">x</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">df</span><span class="o">.</span><span class="n">columns</span> <span class="k">if</span> <span class="s">&#39;Total population&#39;</span> <span class="ow">in</span> <span class="n">x</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">per_100</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">to_numeric</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="n">population</span><span class="p">],</span> <span class="n">errors</span><span class="o">=</span><span class="s">&#39;coerce&#39;</span><span class="p">)</span><span class="o">/</span><span class="mf">100000.0</span>
    <span class="n">df</span><span class="p">[</span><span class="s">&#39;Murders_per_100000&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s">&#39;Murder_and_nonnegligent_manslaughter&#39;</span><span class="p">]</span><span class="o">/</span><span class="n">per_100</span>
    <span class="n">df</span><span class="p">[</span><span class="s">&#39;population&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="n">population</span><span class="p">]</span>
    <span class="c"># Compute number of firearms recovered and sourced in each MSA state</span>
    <span class="k">if</span> <span class="nb">str</span><span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="mi">2006</span><span class="p">)</span> <span class="ow">in</span> <span class="n">atf_dict</span><span class="o">.</span><span class="n">keys</span><span class="p">():</span>
        <span class="n">atf_df</span> <span class="o">=</span> <span class="n">atf_dict</span><span class="p">[</span><span class="nb">str</span><span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="mi">2006</span><span class="p">)]</span>
        <span class="n">msa</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s">&#39;Metropolitan_Statistical_Area&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s">&#39;,&#39;</span><span class="p">)[</span><span class="mi">1</span><span class="p">:][</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">split</span><span class="p">())</span>
        <span class="n">df</span><span class="p">[</span><span class="s">&#39;firearms_recovered&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">msa</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">get_atf_vals</span><span class="p">,</span> <span class="n">atf_df</span><span class="o">=</span><span class="n">atf_df</span><span class="p">,</span> <span class="n">recover</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
        <span class="n">df</span><span class="p">[</span><span class="s">&#39;firearms_sourced&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">msa</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">get_atf_vals</span><span class="p">,</span> <span class="n">atf_df</span><span class="o">=</span><span class="n">atf_df</span><span class="p">,</span> <span class="n">recover</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">df</span><span class="p">[</span><span class="s">&#39;firearms_recovered&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>        
        <span class="n">df</span><span class="p">[</span><span class="s">&#39;firearms_sourced&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
        <span class="c"># Drop any columns that don&#39;t have murder counts</span>
    <span class="n">df</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">dropna</span><span class="p">(</span><span class="n">subset</span><span class="o">=</span><span class="p">[</span><span class="s">&#39;Murders_per_100000&#39;</span><span class="p">])</span>
    <span class="n">df_dict</span><span class="p">[</span><span class="n">i</span> <span class="o">+</span> <span class="mi">6</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span></code></pre></div>
<div class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">frames</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">df_dict</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
    <span class="n">male</span> <span class="o">=</span> <span class="p">[</span><span class="n">x</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">v</span><span class="o">.</span><span class="n">columns</span> <span class="k">if</span> <span class="s">&#39;Male&#39;</span> <span class="ow">in</span> <span class="n">x</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">median_age</span> <span class="o">=</span> <span class="p">[</span><span class="n">x</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">v</span><span class="o">.</span><span class="n">columns</span> <span class="k">if</span> <span class="s">&#39;Median age&#39;</span> <span class="ow">in</span> <span class="n">x</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">median_income</span> <span class="o">=</span> <span class="p">[</span><span class="n">x</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">v</span><span class="o">.</span><span class="n">columns</span> <span class="k">if</span> <span class="s">&#39;Median income&#39;</span> <span class="ow">in</span> <span class="n">x</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">black</span> <span class="o">=</span> <span class="p">[</span><span class="n">x</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">v</span><span class="o">.</span><span class="n">columns</span> <span class="k">if</span> <span class="s">&#39;Black or&#39;</span> <span class="ow">in</span> <span class="n">x</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">white</span> <span class="o">=</span> <span class="p">[</span><span class="n">x</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">v</span><span class="o">.</span><span class="n">columns</span> <span class="k">if</span> <span class="s">&#39;White&#39;</span> <span class="ow">in</span> <span class="n">x</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">hispanic</span> <span class="o">=</span> <span class="p">[</span><span class="n">x</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">v</span><span class="o">.</span><span class="n">columns</span> <span class="k">if</span> <span class="s">&#39;Hispanic&#39;</span> <span class="ow">in</span> <span class="n">x</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">poverty</span> <span class="o">=</span> <span class="p">[</span><span class="n">x</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">v</span><span class="o">.</span><span class="n">columns</span> <span class="k">if</span> <span class="s">&#39;poverty&#39;</span> <span class="ow">in</span> <span class="n">x</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">income_levels</span> <span class="o">=</span> <span class="p">[</span><span class="n">x</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">v</span><span class="o">.</span><span class="n">columns</span> <span class="k">if</span> <span class="s">&#39;Population 15 years and over - $&#39;</span> <span class="ow">in</span> <span class="n">x</span><span class="p">]</span>
    <span class="n">income_labels</span> <span class="o">=</span> <span class="p">[</span><span class="n">x</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s">&quot;- &quot;</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">v</span><span class="o">.</span><span class="n">columns</span> <span class="k">if</span> <span class="s">&#39;Population 15 years and over - $&#39;</span> <span class="ow">in</span> <span class="n">x</span><span class="p">]</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">income_levels</span><span class="p">)):</span>
        <span class="n">v</span><span class="p">[</span><span class="n">income_labels</span><span class="p">[</span><span class="n">i</span><span class="p">]]</span> <span class="o">=</span> <span class="n">v</span><span class="p">[</span><span class="n">income_levels</span><span class="p">[</span><span class="n">i</span><span class="p">]]</span>
    <span class="n">uneducated</span> <span class="o">=</span> <span class="p">[</span><span class="n">x</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">v</span><span class="o">.</span><span class="n">columns</span> <span class="k">if</span> <span class="s">&#39;EDUCATIONAL ATTAINMENT&#39;</span> <span class="ow">in</span> <span class="n">x</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">highschool</span> <span class="o">=</span> <span class="p">[</span><span class="n">x</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">v</span><span class="o">.</span><span class="n">columns</span> <span class="k">if</span> <span class="s">&#39;EDUCATIONAL ATTAINMENT&#39;</span> <span class="ow">in</span> <span class="n">x</span><span class="p">][</span><span class="mi">2</span><span class="p">]</span>
    <span class="n">college</span> <span class="o">=</span> <span class="p">[</span><span class="n">x</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">v</span><span class="o">.</span><span class="n">columns</span> <span class="k">if</span> <span class="s">&#39;EDUCATIONAL ATTAINMENT&#39;</span> <span class="ow">in</span> <span class="n">x</span><span class="p">][</span><span class="mi">3</span><span class="p">:</span><span class="mi">5</span><span class="p">]</span>
    <span class="n">v</span><span class="p">[</span><span class="s">&quot;Year&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="mi">2000</span> <span class="o">+</span> <span class="n">k</span>
    <span class="n">non_native_series</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">to_numeric</span><span class="p">(</span><span class="n">v</span><span class="p">[</span><span class="s">&#39;non_native&#39;</span><span class="p">],</span> <span class="n">errors</span><span class="o">=</span><span class="s">&#39;coerce&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">fillna</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">pop_series</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">to_numeric</span><span class="p">(</span><span class="n">v</span><span class="p">[</span><span class="s">&#39;Total; Estimate; Total population&#39;</span><span class="p">],</span> <span class="n">errors</span><span class="o">=</span><span class="s">&#39;coerce&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">fillna</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">cols</span> <span class="o">=</span> <span class="p">[</span><span class="s">&#39;Murder_and_nonnegligent_manslaughter&#39;</span><span class="p">,</span> <span class="s">&#39;Murders_per_100000&#39;</span><span class="p">,</span> <span class="s">&#39;Year&#39;</span><span class="p">,</span> <span class="n">poverty</span><span class="p">,</span> <span class="n">male</span><span class="p">,</span>
            <span class="n">median_age</span><span class="p">,</span> <span class="n">median_income</span><span class="p">,</span> <span class="n">black</span><span class="p">,</span> <span class="n">white</span><span class="p">,</span> <span class="n">hispanic</span><span class="p">,</span> <span class="n">uneducated</span><span class="p">,</span> <span class="n">highschool</span><span class="p">]</span> <span class="o">+</span> <span class="n">income_levels</span>
    <span class="n">df_temp</span> <span class="o">=</span> <span class="n">v</span><span class="p">[</span><span class="n">cols</span><span class="p">]</span>
    <span class="n">column_labels</span> <span class="o">=</span> <span class="p">[</span><span class="s">&#39;murder_count&#39;</span><span class="p">,</span> <span class="s">&#39;murder_per_100000&#39;</span><span class="p">,</span> <span class="s">&#39;year&#39;</span><span class="p">,</span> <span class="s">&#39;poverty&#39;</span><span class="p">,</span> <span class="s">&#39;male&#39;</span><span class="p">,</span><span class="s">&#39;median_age&#39;</span><span class="p">,</span>
                       <span class="s">&#39;median_income&#39;</span><span class="p">,</span> <span class="s">&#39;black&#39;</span><span class="p">,</span> <span class="s">&#39;white&#39;</span><span class="p">,</span> <span class="s">&#39;hispanic&#39;</span><span class="p">,</span> <span class="s">&#39;uneducated&#39;</span><span class="p">,</span> <span class="s">&#39;highschool&#39;</span><span class="p">]</span> <span class="o">+</span> <span class="n">income_labels</span>
    <span class="n">df_temp</span><span class="o">.</span><span class="n">columns</span> <span class="o">=</span> <span class="n">column_labels</span>
    <span class="n">df_temp</span><span class="p">[</span><span class="s">&#39;college&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">v</span><span class="p">[</span><span class="n">college</span><span class="p">]</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">df_temp</span><span class="p">[</span><span class="s">&#39;foreign&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">non_native_series</span> <span class="o">/</span> <span class="n">pop_series</span>
    <span class="n">df_temp</span><span class="p">[</span><span class="s">&#39;population&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">v</span><span class="p">[</span><span class="s">&#39;population&#39;</span><span class="p">]</span>
    <span class="n">df_temp</span><span class="p">[[</span><span class="s">&#39;firearms_sourced&#39;</span><span class="p">,</span> <span class="s">&#39;firearms_recovered&#39;</span><span class="p">]]</span> <span class="o">=</span> <span class="n">v</span><span class="p">[[</span><span class="s">&#39;firearms_sourced&#39;</span><span class="p">,</span> <span class="s">&#39;firearms_recovered&#39;</span><span class="p">]]</span>
    <span class="n">frames</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">df_temp</span><span class="p">)</span>
<span class="n">df_concat</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">concat</span><span class="p">(</span><span class="n">frames</span><span class="p">)</span></code></pre></div>
<h4 id="linear-ridge-lasso-and-knn-regressors-trained-on-all-columns-including-atf">3.3.2 Linear, Ridge, Lasso, and KNN Regressors trained on all columns including ATF</h4>
<p>Similar to the methodology in 3.1.5 were we include all columns, we train and validate the four models in the defined function with the two added predictor variables, but the results were unfortunately the same as those of the 3.1.5 models. Again, all three linear models returned the same performance scores of training R-squared of 0.45 and validation R-squared of 0.41, and the k-NN had scores of 0.51 and 0.44 for train and validation. Nonetheless, we were still believe applying the subselection methods from 3.2 with the added predictors would bring more insights and potentially enhancements.</p>
<div class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">atf_all_model</span> <span class="o">=</span> <span class="n">train_and_validate</span><span class="p">(</span><span class="n">X_tr</span><span class="p">,</span> <span class="n">X_val</span><span class="p">,</span> <span class="n">y_tr</span><span class="p">,</span> <span class="n">y_val</span><span class="p">,</span> <span class="n">X</span><span class="o">.</span><span class="n">columns</span><span class="p">,</span>
                                   <span class="s">&quot;All Columns Including ATF&quot;</span><span class="p">,</span> <span class="n">add_const</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">poly</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span></code></pre></div>
<pre><code>Training R^2 for All Columns Including ATF Linear Regression: 0.4462319942491597  
Validation R^2 for All Columns Including ATF Linear Regression: 0.4108502357189554  
Training R^2 for All Columns Including ATF Lasso Regression: 0.44560209039581394  
Validation R^2 for All Columns Including ATF Lasso Regression: 0.4102549436785793  
Training R^2 for All Columns Including ATF Ridge Regression: 0.4458059433121516  
Validation R^2 for All Columns Including ATF Ridge Regression: 0.41016795215679747  
Training Score for KNN (k=18) Model: 0.5069220726159709  
Validation Score for KNN (k=18) Model: 0.44440789682622034  
</code></pre>
<h4 id="ols-model-to-determine-statistical-significance-of-additional-atf-columns">3.3.3 OLS Model to determine statistical significance of additional ATF columns</h4>
<div class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">y_atf</span> <span class="o">=</span> <span class="n">df_concat</span><span class="p">[</span><span class="s">&#39;murder_per_100000&#39;</span><span class="p">]</span>
<span class="n">X_atf</span> <span class="o">=</span> <span class="n">df_concat</span><span class="o">.</span><span class="n">drop</span><span class="p">([</span><span class="s">&#39;murder_per_100000&#39;</span><span class="p">,</span> <span class="s">&#39;murder_count&#39;</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">X_atf</span> <span class="o">=</span> <span class="n">X_atf</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">pd</span><span class="o">.</span><span class="n">to_numeric</span><span class="p">,</span> <span class="n">errors</span><span class="o">=</span><span class="s">&#39;coerce&#39;</span><span class="p">)</span>
<span class="n">X_atf_train</span><span class="p">,</span> <span class="n">X_atf_test</span><span class="p">,</span> <span class="n">y_atf_train</span><span class="p">,</span> <span class="n">y_atf_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X_atf</span><span class="p">,</span> <span class="n">y_atf</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span></code></pre></div>
<div class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">X_tr</span><span class="p">,</span> <span class="n">X_val</span><span class="p">,</span> <span class="n">y_tr</span><span class="p">,</span> <span class="n">y_val</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X_atf_train</span><span class="p">,</span> <span class="n">y_atf_train</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span></code></pre></div>
<div class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">imputer</span> <span class="o">=</span> <span class="n">Imputer</span><span class="p">(</span><span class="n">strategy</span><span class="o">=</span><span class="s">&#39;mean&#39;</span><span class="p">)</span>
<span class="n">X_all_train</span> <span class="o">=</span> <span class="n">imputer</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X_tr</span><span class="p">)</span>
<span class="n">X_all_val</span> <span class="o">=</span> <span class="n">imputer</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X_val</span><span class="p">)</span>
<span class="n">scaler</span> <span class="o">=</span> <span class="n">MinMaxScaler</span><span class="p">()</span>
<span class="n">X_const_train</span> <span class="o">=</span> <span class="n">sm</span><span class="o">.</span><span class="n">add_constant</span><span class="p">(</span><span class="n">scaler</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X_all_train</span><span class="p">))</span>
<span class="n">X_const_val</span> <span class="o">=</span> <span class="n">sm</span><span class="o">.</span><span class="n">add_constant</span><span class="p">(</span><span class="n">scaler</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X_all_val</span><span class="p">))</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">OLS</span><span class="p">(</span><span class="n">endog</span><span class="o">=</span><span class="n">y_tr</span><span class="p">,</span> <span class="n">exog</span><span class="o">=</span><span class="n">X_const_train</span><span class="p">)</span>
<span class="n">result</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
<span class="n">y_hat_train</span> <span class="o">=</span> <span class="n">result</span><span class="o">.</span><span class="n">predict</span><span class="p">()</span>
<span class="n">y_hat_val</span> <span class="o">=</span> <span class="n">result</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">exog</span><span class="o">=</span><span class="n">X_const_val</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">&#39;Train R^2 =&#39;</span><span class="p">,</span> <span class="n">r2_score</span><span class="p">(</span><span class="n">y_tr</span><span class="p">,</span> <span class="n">y_hat_train</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="s">&#39;Validation R^2 =&#39;</span><span class="p">,</span> <span class="n">r2_score</span><span class="p">(</span><span class="n">y_val</span><span class="p">,</span> <span class="n">y_hat_val</span><span class="p">))</span></code></pre></div>
<pre><code>Train R^2 = 0.447551488088  
Validation R^2 = 0.409375635927  
</code></pre>
<div class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">xlabs</span> <span class="o">=</span> <span class="p">[</span><span class="s">&#39;const&#39;</span><span class="p">]</span> <span class="o">+</span> <span class="nb">list</span><span class="p">(</span><span class="n">X_tr</span><span class="o">.</span><span class="n">columns</span><span class="p">)</span>
<span class="n">result</span><span class="o">.</span><span class="n">summary</span><span class="p">(</span><span class="n">xname</span><span class="o">=</span><span class="n">xlabs</span><span class="p">)</span></code></pre></div>
<table class="simpletable">
<caption>OLS Regression Results</caption>
<tr>
  <th>Dep. Variable:</th>    <td>murder_per_100000</td> <th>  R-squared:         </th> <td>   0.448</td>
</tr>
<tr>
  <th>Model:</th>                   <td>OLS</td>        <th>  Adj. R-squared:    </th> <td>   0.441</td>
</tr>
<tr>
  <th>Method:</th>             <td>Least Squares</td>   <th>  F-statistic:       </th> <td>   73.09</td>
</tr>
<tr>
  <th>Date:</th>             <td>Wed, 06 Dec 2017</td>  <th>  Prob (F-statistic):</th> <td>3.84e-247</td>
</tr>
<tr>
  <th>Time:</th>                 <td>20:16:04</td>      <th>  Log-Likelihood:    </th> <td> -4717.4</td>
</tr>
<tr>
  <th>No. Observations:</th>      <td>  2099</td>       <th>  AIC:               </th> <td>   9483.</td>
</tr>
<tr>
  <th>Df Residuals:</th>          <td>  2075</td>       <th>  BIC:               </th> <td>   9618.</td>
</tr>
<tr>
  <th>Df Model:</th>              <td>    23</td>       <th>                     </th>     <td> </td>    
</tr>
<tr>
  <th>Covariance Type:</th>      <td>nonrobust</td>     <th>                     </th>     <td> </td>    
</tr>
</table>
<table class="simpletable">
<tr>
            <td></td>              <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P&gt;|t|</th>  <th>[0.025</th>    <th>0.975]</th>  
</tr>
<tr>
  <th>const</th>                <td>    6.0180</td> <td>    1.962</td> <td>    3.068</td> <td> 0.002</td> <td>    2.171</td> <td>    9.865</td>
</tr>
<tr>
  <th>$1 to $9,999 or loss</th> <td>    0.6171</td> <td>    0.794</td> <td>    0.778</td> <td> 0.437</td> <td>   -0.939</td> <td>    2.174</td>
</tr>
<tr>
  <th>$10,000 to $14,999</th>   <td>    0.5167</td> <td>    0.596</td> <td>    0.867</td> <td> 0.386</td> <td>   -0.652</td> <td>    1.686</td>
</tr>
<tr>
  <th>$15,000 to $24,999</th>   <td>   -0.5391</td> <td>    0.906</td> <td>   -0.595</td> <td> 0.552</td> <td>   -2.316</td> <td>    1.238</td>
</tr>
<tr>
  <th>$25,000 to $34,999</th>   <td>    1.1126</td> <td>    0.616</td> <td>    1.806</td> <td> 0.071</td> <td>   -0.096</td> <td>    2.321</td>
</tr>
<tr>
  <th>$35,000 to $49,999</th>   <td>    0.5484</td> <td>    0.673</td> <td>    0.815</td> <td> 0.415</td> <td>   -0.771</td> <td>    1.868</td>
</tr>
<tr>
  <th>$50,000 to $64,999</th>   <td>   -1.0270</td> <td>    0.726</td> <td>   -1.415</td> <td> 0.157</td> <td>   -2.451</td> <td>    0.397</td>
</tr>
<tr>
  <th>$65,000 to $74,999</th>   <td>    1.1053</td> <td>    0.642</td> <td>    1.722</td> <td> 0.085</td> <td>   -0.154</td> <td>    2.364</td>
</tr>
<tr>
  <th>$75,000 or more</th>      <td>    1.7419</td> <td>    1.044</td> <td>    1.669</td> <td> 0.095</td> <td>   -0.305</td> <td>    3.788</td>
</tr>
<tr>
  <th>black</th>                <td>    3.7867</td> <td>    0.491</td> <td>    7.715</td> <td> 0.000</td> <td>    2.824</td> <td>    4.749</td>
</tr>
<tr>
  <th>college</th>              <td>    1.6117</td> <td>    0.868</td> <td>    1.856</td> <td> 0.064</td> <td>   -0.091</td> <td>    3.315</td>
</tr>
<tr>
  <th>firearms_recovered</th>   <td>    2.3063</td> <td>    1.162</td> <td>    1.984</td> <td> 0.047</td> <td>    0.027</td> <td>    4.586</td>
</tr>
<tr>
  <th>firearms_sourced</th>     <td>   -2.1071</td> <td>    0.963</td> <td>   -2.188</td> <td> 0.029</td> <td>   -3.995</td> <td>   -0.219</td>
</tr>
<tr>
  <th>foreign</th>              <td>   -1.6875</td> <td>    0.472</td> <td>   -3.575</td> <td> 0.000</td> <td>   -2.613</td> <td>   -0.762</td>
</tr>
<tr>
  <th>highschool</th>           <td>    3.2509</td> <td>    0.783</td> <td>    4.150</td> <td> 0.000</td> <td>    1.715</td> <td>    4.787</td>
</tr>
<tr>
  <th>hispanic</th>             <td>    0.8688</td> <td>    0.598</td> <td>    1.453</td> <td> 0.146</td> <td>   -0.304</td> <td>    2.042</td>
</tr>
<tr>
  <th>male</th>                 <td>   -0.6359</td> <td>    0.672</td> <td>   -0.946</td> <td> 0.344</td> <td>   -1.954</td> <td>    0.683</td>
</tr>
<tr>
  <th>median_age</th>           <td>    1.7402</td> <td>    0.707</td> <td>    2.460</td> <td> 0.014</td> <td>    0.353</td> <td>    3.127</td>
</tr>
<tr>
  <th>median_income</th>        <td>   -2.1822</td> <td>    1.188</td> <td>   -1.837</td> <td> 0.066</td> <td>   -4.512</td> <td>    0.148</td>
</tr>
<tr>
  <th>population</th>           <td>    1.7017</td> <td>    0.820</td> <td>    2.076</td> <td> 0.038</td> <td>    0.094</td> <td>    3.309</td>
</tr>
<tr>
  <th>poverty</th>              <td>   -0.0787</td> <td>    0.708</td> <td>   -0.111</td> <td> 0.911</td> <td>   -1.467</td> <td>    1.309</td>
</tr>
<tr>
  <th>uneducated</th>           <td>    3.2177</td> <td>    0.917</td> <td>    3.508</td> <td> 0.000</td> <td>    1.419</td> <td>    5.016</td>
</tr>
<tr>
  <th>white</th>                <td>   -8.8336</td> <td>    0.633</td> <td>  -13.955</td> <td> 0.000</td> <td>  -10.075</td> <td>   -7.592</td>
</tr>
<tr>
  <th>year</th>                 <td>    0.0035</td> <td>    0.207</td> <td>    0.017</td> <td> 0.987</td> <td>   -0.403</td> <td>    0.410</td>
</tr>
</table>
<table class="simpletable">
<tr>
  <th>Omnibus:</th>       <td>514.015</td> <th>  Durbin-Watson:     </th> <td>   2.055</td>
</tr>
<tr>
  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td>2296.064</td>
</tr>
<tr>
  <th>Skew:</th>          <td> 1.105</td>  <th>  Prob(JB):          </th> <td>    0.00</td>
</tr>
<tr>
  <th>Kurtosis:</th>      <td> 7.623</td>  <th>  Cond. No.          </th> <td>    103.</td>
</tr>
</table>
<p><img src="Prediction_model_files/Prediction_model_55_1.png" alt="png" /></p>
<h4 id="models-trained-on-most-significant-predictors-determined-by-the-ols-model-with-atf-columns">3.3.4 Models trained on most significant predictors determined by the OLS Model with ATF Columns</h4>
<p>Just as we run an OLS model in 3.2.1 to determine statistically significant predictors on all columns, both of the added ATF columns were, in fact, selected as two of the nine statistically significant variables: alongside the constant/slope, Black/African American demographic, highschool or equivalent attainment, median age, total population, less than high school demographic, and White demographic predictors. The fact that both of the new columns had p-values less than the alpha of 0.05 speaks volume to the high correlative relationship between firearms and crime that was previously not being considered.</p>
<p>Running the same train and validation modeling methodology, we obtained similar performance results as from the models above in 3.3.2. The three linear regressions had train and validation scores of 0.44 and 0.41, respectively; and the k-NN model with best k of 20 had marginal improvements in its performance scores with train R-squared of 0.53 and validation R-squared of 0.45. Still, the k-NN model in 3.2.2 where we trained models on most significant predictors determined by OLS without the ATF columns have the best performance scores.</p>
<div class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">most_significant_atf</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">xlabs</span><span class="p">)[</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">result</span><span class="o">.</span><span class="n">pvalues</span> <span class="o">&lt;</span> <span class="mf">0.05</span><span class="p">)][</span><span class="mi">1</span><span class="p">:]</span>
<span class="n">most_significant_atf</span></code></pre></div>
<pre><code>array(['black', 'firearms_recovered', 'firearms_sourced', 'foreign',
       'highschool', 'median_age', 'population', 'uneducated', 'white'],
      dtype='&lt;U20')
</code></pre>
<div class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">atf_most_significant</span> <span class="o">=</span> <span class="n">train_and_validate</span><span class="p">(</span><span class="n">X_tr</span><span class="p">,</span> <span class="n">X_val</span><span class="p">,</span> <span class="n">y_tr</span><span class="p">,</span> <span class="n">y_val</span><span class="p">,</span> <span class="n">most_significant_atf</span><span class="p">,</span>
                                   <span class="s">&quot;Most Significant Columns as Determined by OLS Including ATF&quot;</span><span class="p">,</span>
                                    <span class="n">add_const</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">poly</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span></code></pre></div>
<pre><code>Training R^2 for Most Significant Columns as Determined by OLS Including ATF Linear Regression: 0.4416844727031795  
Validation R^2 for Most Significant Columns as Determined by OLS Including ATF Linear Regression: 0.4055867039068164  
Training R^2 for Most Significant Columns as Determined by OLS Including ATF Lasso Regression: 0.4416569916105543  
Validation R^2 for Most Significant Columns as Determined by OLS Including ATF Lasso Regression: 0.405664008332985  
Training R^2 for Most Significant Columns as Determined by OLS Including ATF Ridge Regression: 0.44144209747384194  
Validation R^2 for Most Significant Columns as Determined by OLS Including ATF Ridge Regression: 0.4057770445145181  
Training Score for KNN (k=20) Model: 0.5339733103141981  
Validation Score for KNN (k=20) Model: 0.44500450313796264  
</code></pre>
<p><img src="Prediction_model_files/Prediction_model_58_1.png" alt="png" /></p>
<h4 id="models-trained-on-most-significant-predictors-with-polynomial-and-interaction-variables">3.3.5 Models trained on most significant predictors with polynomial and interaction variables</h4>
<p>Now, we used PolynomialFeatures to create fourth degree polynomial predictors for the aforementioned statistically significant columns in 3.3.3 along with interaction variables between them. Then, we used this to train and validate our four models. Similar to 3.3.3, the multinomial regression overfits with negative validation score, the Lasso and Ridge regressions both improve with higher train/validation scores of 0.50/0.45 and 0.53/0.44, and finally the k-NN regression with best k of 27, surprisingly, does not overfit and returns slightly improved train and validation scores of 0.52 and 0.46.</p>
<p>Although the k-NN model did not improve with the addition of the ATF data, we were able to obtain comparable performance score with our Lasso regression. Compared to the highest train and validation R-squared values of 0.45 and 0.41 without ATF columns in 3.2, the new Lasso regression with the two ATF columns does have around 0.05 increase for the scores.</p>
<div class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">atf_most_sig_poly_model</span> <span class="o">=</span> <span class="n">train_and_validate</span><span class="p">(</span><span class="n">X_tr</span><span class="p">,</span> <span class="n">X_val</span><span class="p">,</span> <span class="n">y_tr</span><span class="p">,</span> <span class="n">y_val</span><span class="p">,</span> <span class="n">most_significant_atf</span><span class="p">,</span>
                                   <span class="s">&quot;All Columns Including ATF with Polynomial Predictors&quot;</span><span class="p">,</span> <span class="n">add_const</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">poly</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span></code></pre></div>
<pre><code>Training R^2 for All Columns Including ATF with Polynomial Predictors Linear Regression: 0.7320519103314165  
Validation R^2 for All Columns Including ATF with Polynomial Predictors Linear Regression: -304.9367042667425  
Training R^2 for All Columns Including ATF with Polynomial Predictors Lasso Regression: 0.49935261726706975  
Validation R^2 for All Columns Including ATF with Polynomial Predictors Lasso Regression: 0.450994478817581  
Training R^2 for All Columns Including ATF with Polynomial Predictors Ridge Regression: 0.5336746781783491  
Validation R^2 for All Columns Including ATF with Polynomial Predictors Ridge Regression: 0.44413048096274194  
Training Score for KNN (k=27) Model: 0.5172479091867096  
Validation Score for KNN (k=27) Model: 0.4618831040144915  
</code></pre>
<p><img src="Prediction_model_files/Prediction_model_60_3.png" alt="png" /></p>
<h4 id="forwards-and-backwards-subset-selection">3.3.6 Forwards and Backwards subset selection</h4>
<p>When we ran forwards and backwards selection methods with the two new columns, both direction fails to select either of the ATF columns. In fact, the selection methods choose the same columns as in 3.2.3 stepwise selection; and, therefore, we are not going to create a model here.</p>
<div class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">predictors_forward</span> <span class="o">=</span> <span class="n">step_forwards_backwards</span><span class="p">(</span><span class="n">X_tr</span><span class="p">,</span> <span class="n">y_tr</span><span class="p">,</span> <span class="n">direction</span><span class="o">=</span><span class="s">&#39;forward&#39;</span><span class="p">)</span>
<span class="n">predictors_forward</span></code></pre></div>
<pre><code>['highschool', 'black', 'median_age', 'foreign', 'uneducated', 'white']  
</code></pre>
<div class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">predictors_backward</span> <span class="o">=</span> <span class="n">step_forwards_backwards</span><span class="p">(</span><span class="n">X_tr</span><span class="p">,</span> <span class="n">y_tr</span><span class="p">,</span> <span class="n">direction</span><span class="o">=</span><span class="s">&#39;backward&#39;</span><span class="p">)</span>
<span class="n">predictors_backward</span></code></pre></div>
<pre><code>['median_age', 'white', 'highschool', 'black', 'foreign', 'uneducated']  
</code></pre>
<h2 id="testing-the-final-2-models">4 Testing the Final 2 models</h2>
<h3 id="lasso-regression-with-atf-data-using-predictors-with-p-stat--005-and-polynomial-features">4.1 Lasso Regression with ATF data, using predictors with p stat &lt; 0.05 and polynomial features</h3>
<p>The Lasso regression in 3.3.4, using statistically significant predictors transformed with PolynomialFeatures on the ATF included data, had train and validation scores of 0.52 and 0.46. This model had a test R-squared value of 0.48, which is frankly higher than what we had imagined during our EDA.</p>
<div class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">X_lasso_atf_final</span> <span class="o">=</span> <span class="n">imputer</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X_atf_test</span><span class="p">[</span><span class="n">most_significant_atf</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">)</span>
<span class="n">scaler</span> <span class="o">=</span> <span class="n">MinMaxScaler</span><span class="p">()</span>
<span class="n">poly_generator</span> <span class="o">=</span> <span class="n">PolynomialFeatures</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="n">include_bias</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
<span class="n">X_lasso_atf_final</span> <span class="o">=</span> <span class="n">scaler</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X_lasso_atf_final</span><span class="p">)</span>
<span class="n">X_lasso_atf_final</span> <span class="o">=</span> <span class="n">poly_generator</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X_lasso_atf_final</span><span class="p">)</span></code></pre></div>
<div class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">final_lasso_model</span> <span class="o">=</span> <span class="n">atf_most_sig_poly_model</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span></code></pre></div>
<div class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">print</span><span class="p">(</span><span class="s">&quot;R^2 on the test set: {}&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">final_lasso_model</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_lasso_atf_final</span><span class="p">,</span> <span class="n">y_atf_test</span><span class="p">)))</span></code></pre></div>
<pre><code>R^2 on the test set: 0.47946491031795335  
</code></pre>
<h3 id="knn-regressor-with-forwards-and-backwards-selected-predictors">4.2 KNN Regressor with forwards and backwards selected predictors</h3>
<p>The k-NN regressor in 3.2.4 with best k of 7, using the BIC-based stepwise selected predictors in 3.2.2, had train and validation scores of 0.66 and 0.50. The model had a test R-squared value of 0.51, revealing that this may be the most accurate albeit computationally heavy model.</p>
<div class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">X_knn_atf_final</span> <span class="o">=</span> <span class="n">imputer</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X_atf_test</span><span class="p">[</span><span class="n">intersection</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">)</span>
<span class="n">scaler</span> <span class="o">=</span> <span class="n">MinMaxScaler</span><span class="p">()</span>
<span class="n">X_knn_atf_final</span> <span class="o">=</span> <span class="n">sm</span><span class="o">.</span><span class="n">add_constant</span><span class="p">(</span><span class="n">scaler</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X_knn_atf_final</span><span class="p">))</span></code></pre></div>
<div class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">final_knn_model</span> <span class="o">=</span> <span class="n">inter_models</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span></code></pre></div>
<div class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">print</span><span class="p">(</span><span class="s">&quot;R^2 on the test set: {}&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">final_knn_model</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_knn_atf_final</span><span class="p">,</span> <span class="n">y_atf_test</span><span class="p">)))</span></code></pre></div>
<pre><code>R^2 on the test set: 0.512266327944152
</code></pre>
  </div>
</article>
        </section>
<div class="clearfix"></div>
<footer class="site-footer txt-center">
  <hr>
  <ul class="social">
  </ul>
  <small>&copy; 2017 All rights reserved. Made with <a href="http://jekyllrb.com" target="_blank">Jekyll</a> and <i class="icon icon-heart"></i></small>
  <small>by <a href="http://nandomoreira.me" target="_blank">nandomoreira.me</a></small>
</footer>
    </main>
    <a href="" target="_blank" class="github-corner"><svg width="80" height="80" viewBox="0 0 250 250" style="fill:#2ecc71; color:#f7f8f9; position: absolute; top: 0; border: 0; right: 0;"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>
  </body>
</html>
